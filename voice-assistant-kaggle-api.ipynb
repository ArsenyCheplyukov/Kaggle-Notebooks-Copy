{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#creating venv\n!python -m venv env\n!source ./env/bin/activate\n#libs installation\n!pip install -q pydub\n!pip install -q sounddevice\n!pip install -q rpunct\n!pip install -q torch==1.7.0\n!pip install -q torchaudio==0.7.0\n!apt-get install -y libsndfile1 ffmpeg\n!pip install -q Cython\n!pip install -q -U nemo_toolkit[all]\n!pip uninstall -q scikit-learn -y\n!pip install -q scikit-learn==0.23.2\n!pip install -q tslearn\n!git clone https://github.com/NVIDIA/apex.git\n!python3 ./apex/setup.py install\n!pip install -q transformers[torch]\n!pip install -q playsound pyaudio pydub ffmpeg-python\n\n# command list\ncommands = [\"activate\",\n            \"bot\",\n            \"story\",\n            \"stop\"]\nnumbers = {\"one\" : 1,\n           \"two\" : 2,\n           \"three\" : 3,\n           \"four\" : 4,\n           \"five\" : 5,\n           \"six\" : 6,\n           \"seven\" : 7,\n           \"eight\" : 8,\n           \"nine\" : 9}\n\nprobability_of_command = 0.5\n\nfs = 16000  # Sample rate\nseconds = 1  # Duration of recording\n\nfrom difflib import SequenceMatcher\nimport torch\nimport soundfile as sf\nfrom nemo.collections.tts.models.base import SpectrogramGenerator, Vocoder\nfrom nemo.collections.asr.models import ASRModel, EncDecClassificationModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GPTNeoForCausalLM, GPT2Tokenizer, AutoModelWithLMHead, AutoTokenizer\nfrom rpunct import RestorePuncts\nfrom time import time\nimport sounddevice as sd\nfrom scipy.io.wavfile import write\nfrom pydub import AudioSegment\nfrom playsound import playsound\n\ndef checkCommand(seq):\n    prob = [SequenceMatcher(None, seq, seq2).ratio() for seq2 in commands+list(numbers.keys())]\n    max_elem = max(prob)\n    return [commands[i] for i, j in enumerate(prob) if j == max_elem] if max_elem>=probability_of_command else [\"\"]\n\ndef recordVoice():\n    !rm -R rec\n    !mkdir rec\n    \n    !cp ../input/voiceexamples/empty.wav ./rec/main.wav\n    !cp ../input/voiceexamples/empty.wav ./rec/trash.wav\n    \n    playsound('../input/voiceexamples/activation.wav')\n    print(\"RECORDING\")\n    playsound('../input/voiceexamples/beep.wav')\n    \n    while not (torch.is_nonzero(activityDetector.transcribe(paths2audio_files=[\"./rec/trash.wav\"], logprobs=False)[0])):\n        myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=1)\n        sd.wait()  # Wait until recording is finished\n        write('./rec/trash.wav', fs, myrecording)  # Save as WAV file \n    while(torch.is_nonzero(activityDetector.transcribe(paths2audio_files=[\"./rec/trash.wav\"], logprobs=False)[0])):\n        main = AudioSegment.from_file(\"./rec/main.wav\", format=\"wav\")\n        newActivity = AudioSegment.from_file(\"./rec/trash.wav\", format=\"wav\")\n        combined = main + newActivity\n        handle = combined.export(\"./rec/main.wav\", format=\"wav\")\n        myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=1)\n        sd.wait()  # Wait until recording is finished\n        write('./rec/trash.wav', fs, myrecording)  # Save as WAV file \n        \n    playsound('../input/voiceexamples/beep.wav')\n        \ngenerator = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\").to(\"cuda\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n\nrecognizer = ASRModel.from_pretrained(model_name=\"QuartzNet15x5Base-En\").cuda().eval()\n\ngenerator_voice = SpectrogramGenerator.from_pretrained(\"tts_en_fastpitch\", strict=False).cuda().eval()\nvocoder = Vocoder.from_pretrained(\"tts_hifigan\", strict=False).cuda().eval()\n\nbot_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\nbot = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\").to(\"cuda\")\n\nactivityDetector = EncDecClassificationModel.from_pretrained(model_name=\"MarbleNet-3x2x64\").cuda().eval()\n\n#translator = AutoModelWithLMHead.from_pretrained(\"t5-base\").to(\"cuda\")\n#translator_tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n\nrpunct = RestorePuncts()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T12:32:14.386108Z","iopub.execute_input":"2021-08-28T12:32:14.386434Z","iopub.status.idle":"2021-08-28T12:37:25.489397Z","shell.execute_reply.started":"2021-08-28T12:32:14.386359Z","shell.execute_reply":"2021-08-28T12:37:25.486899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recordVoice()\ncom = checkCommand(recognizer.transcribe(paths2audio_files=[\"./rec/main.wav\"], logprobs=False)[0])\nif \"activate\" in com:\n    while(True):\n        recordVoice()\n        voice = checkCommand(recognizer.transcribe(paths2audio_files=[\"./rec/main.wav\"], logprobs=False)[0])\n        if \"bot\" in command:\n            recordVoice()\n            if x in list(numbers.keys()) for x in checkCommand(recognizer.transcribe(paths2audio_files=[\"./rec/main.wav\"], logprobs=False)[0]):\n                for step in range(numbers[x]):\n                    recordVoice()\n                    voice = rpunct.punctuate(recognizer.transcribe(paths2audio_files=inp_file, logprobs=False)[0])\n                    new_user_input_ids = bot_tokenizer.encode(voice + bot_tokenizer.eos_token, return_tensors='pt').to(\"cuda\")\n                    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n                    chat_history_ids = bot.generate(\n                        bot_input_ids,\n                        max_length=1024,\n                        #num_beams=5, # 3\n                        #do_sample=True,\n                        top_k=0, #100\n                        temperature=0.75,\n                        top_p=0.95,\n                        early_stopping=True,\n                        pad_token_id=tokenizer.eos_token_id)\n                    answers = \"Response: {}\".format(bot_tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:].cpu()[0], skip_special_tokens=True))\n                    audio = vocoder.convert_spectrogram_to_audio(\n                                spec=generator_voice.generate_spectrogram(\n                                    tokens=generator_voice.parse(answers)\n                                )\n                            )\n                    sf.write(\"speech.wav\", audio.cpu().detach().numpy()[0], 22050)\n                    playsound('./speech.wav')\n        elif \"story\" in command:\n            recordVoice()\n            voice = rpunct.punctuate(recognizer.transcribe(paths2audio_files=[\"./rec/main.wav\"], logprobs=False)[0])\n            generated = tokenizer.decode(generator.generate(tokenizer(voice, return_tensors=\"pt\").input_ids.to(\"cuda\"), do_sample=True, temperature=0.9, max_length=128).cpu()[0], skip_special_tokens=True)\n            audio = vocoder.convert_spectrogram_to_audio(\n                        spec=generator_voice.generate_spectrogram(\n                            tokens=generator_voice.parse(answers)\n                        )\n                    )\n            sf.write(\"speech.wav\", audio.cpu().detach().numpy()[0], 22050)\n            playsound('./speech.wav')\n        elif \"stop\" in command:\n            return;\n        else:\n            recordVoice()\n            voice = recognizer.transcribe(paths2audio_files=[\"./rec/main.wav\"], logprobs=False)[0]\nelse:\n    recordVoice()\n    com = checkCommand(recognizer.transcribe(paths2audio_files=[\"./rec/main.wav\"], logprobs=False)[0])","metadata":{},"execution_count":null,"outputs":[]}]}