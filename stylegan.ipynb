{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Clear any logs from previous runs\n!rm -rf ./logs/ \n!mkdir ./logs/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From Github Gist: https://gist.github.com/hantoine/4e7c5bc6748861968e61e60bab89e9b0\nfrom urllib.request import urlopen\nfrom io import BytesIO\nfrom zipfile import ZipFile\nfrom subprocess import Popen\nfrom os import chmod\nfrom os.path import isfile\nimport json\nimport time\nimport psutil\n\ndef download_and_unzip(url, extract_to='.'):\n    http_response = urlopen(url)\n    zipfile = ZipFile(BytesIO(http_response.read()))\n    zipfile.extractall(path=extract_to)\n\n\ndef run_cmd_async_unsafe(cmd):\n    return Popen(cmd, shell=True)\n\n\ndef is_process_running(process_name):\n    running_process_names = (proc.name() for proc in psutil.process_iter())\n    return process_name in running_process_names\n\ndef launch_tensorboard():\n    tb_process, ngrok_process = None, None\n    \n    # Launch TensorBoard\n    if not is_process_running('tensorboard'):\n        tb_command = 'tensorboard --logdir ./logs/ --host 0.0.0.0 --port 6006'\n        tb_process = run_cmd_async_unsafe(tb_command)\n    \n    # Install ngrok\n    if not isfile('./ngrok'):\n        ngrok_url = 'https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip'\n        download_and_unzip(ngrok_url)\n        chmod('./ngrok', 0o755)\n\n    # Create ngrok tunnel and print its public URL\n    if not is_process_running('ngrok'):\n        ngrok_process = run_cmd_async_unsafe('./ngrok http 6006')\n        time.sleep(1) # Waiting for ngrok to start the tunnel\n    ngrok_api_res = urlopen('http://127.0.0.1:4040/api/tunnels', timeout=10)\n    ngrok_api_res = json.load(ngrok_api_res)\n    assert len(ngrok_api_res['tunnels']) > 0, 'ngrok tunnel not found'\n    tb_public_url = ngrok_api_res['tunnels'][0]['public_url']\n    print(f'TensorBoard URL: {tb_public_url}')\n\n    return tb_process, ngrok_process\n\ntb_process, ngrok_process = launch_tensorboard()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom math import log2\nimport os\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom multiprocessing import Pool\nimport albumentations as A\nimport cv2\nimport random\nimport numpy as np\nimport torchvision\nimport warnings\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom albumentations.pytorch import ToTensorV2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"START_TRAIN_AT_IMG_SIZE = 32\nDATASET = 'FFHQ_32'\nCHECKPOINT_GEN = \"generator.pth\"\nCHECKPOINT_CRITIC = \"critic.pth\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nLOAD_MODEL = True\nSAVE_MODEL = True\nLEARNING_RATE = 1e-3\nBATCH_SIZES = [32, 32, 32, 32, 32, 16, 8, 4, 2]\nCHANNELS_IMG = 3\nZ_DIM = 512\nW_DIM = 512\nIN_CHANNELS = 512\nLAMBDA_GP = 10\nPROGRESSIVE_EPOCHS = [50] * 100\nFIXED_NOISE = torch.randn((8, Z_DIM)).to(DEVICE)\nNUM_WORKERS = 6","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_to_tensorboard(\n    writer, loss_critic, loss_gen, real, fake, tensorboard_step\n):\n    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n\n    with torch.no_grad():\n        # take out (up to) 32 examples\n        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n    BATCH_SIZE, C, H, W = real.shape\n    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n    interpolated_images = real * beta + fake.detach() * (1 - beta)\n    interpolated_images.requires_grad_(True)\n\n    # Calculate critic scores\n    mixed_scores = critic(interpolated_images, alpha, train_step)\n\n    # Take the gradient of the scores with respect to the images\n    gradient = torch.autograd.grad(\n        inputs=interpolated_images,\n        outputs=mixed_scores,\n        grad_outputs=torch.ones_like(mixed_scores),\n        create_graph=True,\n        retain_graph=True,\n    )[0]\n    gradient = gradient.view(gradient.shape[0], -1)\n    gradient_norm = gradient.norm(2, dim=1)\n    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n    return gradient_penalty","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, filename)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"=> Loading checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # If we don't do this then it will just have learning rate of old checkpoint\n    # and it will lead to many hours of debugging \\:\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EMA:\n    # Found this useful (thanks alexis-jacq):\n    # https://discuss.pytorch.org/t/how-to-apply-exponential-moving-average-decay-for-variables/10856/3\n    def __init__(self, gamma=0.99, save=True, save_frequency=100, save_filename=\"ema_weights.pth\"):\n        \"\"\"\n        Initialize the weight to which we will do the\n        exponential moving average and the dictionary\n        where we store the model parameters\n        \"\"\"\n        self.gamma = gamma\n        self.registered = {}\n        self.save_filename = save_filename\n        self.save_frequency = save_frequency\n        self.count = 0\n\n        if save_filename in os.listdir(\".\"):\n            self.registered = torch.load(self.save_filename)\n\n        if not save:\n            warnings.warn(\"Note that the exponential moving average weights will not be saved to a .pth file!\")\n\n    def register_weights(self, model):\n        \"\"\"\n        Registers the weights of the model which will\n        later be used when we take the moving average\n        \"\"\"\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.registered[name] = param.clone().detach()\n\n    def __call__(self, model):\n        self.count += 1\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                new_weight = param.clone().detach() if name not in self.registered else self.gamma * param + (1 - self.gamma) * self.registered[name]\n                self.registered[name] = new_weight\n\n        if self.count % self.save_frequency == 0:\n            self.save_ema_weights()\n\n    def copy_weights_to(self, model):\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                param.data = self.registered[name]\n\n    def save_ema_weights(self):\n        torch.save(self.registered, self.save_filename)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Z_DIM = 512\nW_DIM = 512\nIN_CHANNELS = 512\ngen = Generator(Z_DIM, W_DIM, IN_CHANNELS, img_channels=3).to(\"cuda\")\ndisc = Discriminator(IN_CHANNELS, img_channels=3).to(\"cuda\")\n\ntot = 0\nfor param in gen.parameters():\n    tot += param.numel()\n\nprint(tot)\nimport sys\nsys.exit()\n\n\nfor img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n    num_steps = int(log2(img_size / 4))\n    x = torch.randn((2, Z_DIM)).to(\"cuda\")\n    z = gen(x, 0.5, steps=num_steps)\n    assert z.shape == (2, 3, img_size, img_size)\n    out = disc(z, alpha=0.5, steps=num_steps)\n    assert out.shape == (2, 1)\n    print(f\"Success! At img size: {img_size}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PixelNorm(nn.Module):\n    def __init__(self):\n        super(PixelNorm, self).__init__()\n        self.epsilon = 1e-8\n\n    def forward(self, x):\n        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MappingNetwork(nn.Module):\n    def __init__(self, z_dim, w_dim):\n        super().__init__()\n        self.mapping = nn.Sequential(\n            PixelNorm(),\n            WSLinear(z_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n            nn.ReLU(),\n            WSLinear(w_dim, w_dim),\n        )\n\n    def forward(self, x):\n        return self.mapping(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InjectNoise(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n\n    def forward(self, x):\n        noise = torch.randn((x.shape[0], 1, x.shape[2], x.shape[3]), device=x.device)\n        return x + self.weight * noise","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AdaIN(nn.Module):\n    def __init__(self, channels, w_dim):\n        super().__init__()\n        self.instance_norm = nn.InstanceNorm2d(channels)\n        self.style_scale = WSLinear(w_dim, channels)\n        self.style_bias = WSLinear(w_dim, channels)\n\n    def forward(self, x, w):\n        x = self.instance_norm(x)\n        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)\n        style_bias = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n        return style_scale * x + style_bias","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WSConv2d(nn.Module):\n    def __init__(\n        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2,\n    ):\n        super(WSConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n        self.bias = self.conv.bias\n        self.conv.bias = None\n\n        # initialize conv layer\n        nn.init.normal_(self.conv.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, x):\n        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WSLinear(nn.Module):\n    def __init__(\n        self, in_features, out_features, gain=2,\n    ):\n        super(WSLinear, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.scale = (gain / in_features)**0.5\n        self.bias = self.linear.bias\n        self.linear.bias = None\n\n        # initialize linear layer\n        nn.init.normal_(self.linear.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, x):\n        return self.linear(x * self.scale) + self.bias","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GenBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, w_dim):\n        super(GenBlock, self).__init__()\n        self.conv1 = WSConv2d(in_channels, out_channels)\n        self.conv2 = WSConv2d(out_channels, out_channels)\n        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n        self.inject_noise1 = InjectNoise(out_channels)\n        self.inject_noise2 = InjectNoise(out_channels)\n        self.adain1 = AdaIN(out_channels, w_dim)\n        self.adain2 = AdaIN(out_channels, w_dim)\n\n    def forward(self, x, w):\n        x = self.adain1(self.leaky(self.inject_noise1(self.conv1(x))), w)\n        x = self.adain2(self.leaky(self.inject_noise2(self.conv2(x))), w)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n        self.conv1 = WSConv2d(in_channels, out_channels)\n        self.conv2 = WSConv2d(out_channels, out_channels)\n        self.leaky = nn.LeakyReLU(0.2)\n\n    def forward(self, x):\n        x = self.leaky(self.conv1(x))\n        x = self.leaky(self.conv2(x))\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_channels, img_channels=3):\n        super(Discriminator, self).__init__()\n        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n        self.leaky = nn.LeakyReLU(0.2)\n\n        # here we work back ways from factors because the discriminator\n        # should be mirrored from the generator. So the first prog_block and\n        # rgb layer we append will work for input size 1024x1024, then 512->256-> etc\n        for i in range(len(factors) - 1, 0, -1):\n            conv_in = int(in_channels * factors[i])\n            conv_out = int(in_channels * factors[i - 1])\n            self.prog_blocks.append(ConvBlock(conv_in, conv_out))\n            self.rgb_layers.append(\n                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n            )\n\n        # perhaps confusing name \"initial_rgb\" this is just the RGB layer for 4x4 input size\n        # did this to \"mirror\" the generator initial_rgb\n        self.initial_rgb = WSConv2d(\n            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.rgb_layers.append(self.initial_rgb)\n        self.avg_pool = nn.AvgPool2d(\n            kernel_size=2, stride=2\n        )  # down sampling using avg pool\n\n        # this is the block for 4x4 input size\n        self.final_block = nn.Sequential(\n            # +1 to in_channels because we concatenate from MiniBatch std\n            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n            nn.LeakyReLU(0.2),\n            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n            nn.LeakyReLU(0.2),\n            WSConv2d(\n                in_channels, 1, kernel_size=1, padding=0, stride=1\n            ),  # we use this instead of linear layer\n        )\n\n    def fade_in(self, alpha, downscaled, out):\n        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n        return alpha * out + (1 - alpha) * downscaled\n\n    def minibatch_std(self, x):\n        batch_statistics = (\n            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n        )\n        # we take the std for each example (across all channels, and pixels) then we repeat it\n        # for a single channel and concatenate it with the image. In this way the discriminator\n        # will get information about the variation in the batch/image\n        return torch.cat([x, batch_statistics], dim=1)\n\n    def forward(self, x, alpha, steps):\n        # where we should start in the list of prog_blocks, maybe a bit confusing but\n        # the last is for the 4x4. So example let's say steps=1, then we should start\n        # at the second to last because input_size will be 8x8. If steps==0 we just\n        # use the final block\n        cur_step = len(self.prog_blocks) - steps\n\n        # convert from rgb as initial step, this will depend on\n        # the image size (each will have it's on rgb layer)\n        out = self.leaky(self.rgb_layers[cur_step](x))\n\n        if steps == 0:  # i.e, image is 4x4\n            out = self.minibatch_std(out)\n            return self.final_block(out).view(out.shape[0], -1)\n\n        # because prog_blocks might change the channels, for down scale we use rgb_layer\n        # from previous/smaller size which in our case correlates to +1 in the indexing\n        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n        out = self.avg_pool(self.prog_blocks[cur_step](out))\n\n        # the fade_in is done first between the downscaled and the input\n        # this is opposite from the generator\n        out = self.fade_in(alpha, downscaled, out)\n\n        for step in range(cur_step + 1, len(self.prog_blocks)):\n            out = self.prog_blocks[step](out)\n            out = self.avg_pool(out)\n\n        out = self.minibatch_std(out)\n        return self.final_block(out).view(out.shape[0], -1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, z_dim, w_dim, in_channels, img_channels=3):\n        super(Generator, self).__init__()\n        self.starting_constant = nn.Parameter(torch.ones((1, in_channels, 4, 4)))\n        self.map = MappingNetwork(z_dim, w_dim)\n        self.initial_adain1 = AdaIN(in_channels, w_dim)\n        self.initial_adain2 = AdaIN(in_channels, w_dim)\n        self.initial_noise1 = InjectNoise(in_channels)\n        self.initial_noise2 = InjectNoise(in_channels)\n        self.initial_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n\n        self.initial_rgb = WSConv2d(\n            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.prog_blocks, self.rgb_layers = (\n            nn.ModuleList([]),\n            nn.ModuleList([self.initial_rgb]),\n        )\n\n        for i in range(len(factors) - 1):  # -1 to prevent index error because of factors[i+1]\n            conv_in_c = int(in_channels * factors[i])\n            conv_out_c = int(in_channels * factors[i + 1])\n            self.prog_blocks.append(GenBlock(conv_in_c, conv_out_c, w_dim))\n            self.rgb_layers.append(\n                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n            )\n\n    def fade_in(self, alpha, upscaled, generated):\n        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n\n    def forward(self, noise, alpha, steps):\n        w = self.map(noise)\n        x = self.initial_adain1(self.initial_noise1(self.starting_constant), w)\n        x = self.initial_conv(x)\n        out = self.initial_adain2(self.leaky(self.initial_noise2(x)), w)\n\n        if steps == 0:\n            return self.initial_rgb(x)\n\n        for step in range(steps):\n            upscaled = F.interpolate(out, scale_factor=2, mode=\"bilinear\")\n            out = self.prog_blocks[step](upscaled, w)\n\n        # The number of channels in upscale will stay the same, while\n        # out which has moved through prog_blocks might change. To ensure\n        # we can convert both to rgb we use different rgb_layers\n        # (steps-1) and steps for upscaled, out respectively\n        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n        final_out = self.rgb_layers[steps](out)\n        return self.fade_in(alpha, final_upscaled, final_out)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_dir = \"FFHQ/images1024x1024/\"\nfiles = os.listdir(root_dir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize(file, size, folder_to_save):\n    image = Image.open(root_dir + file).resize((size, size), Image.LANCZOS)\n    image.save(folder_to_save+file, quality=100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in tqdm(os.listdir(root_dir)):\n    img = Image.open(root_dir+ \"/\"+file).resize((128, 128))\n    img.save(\"FFHQ_resized/\"+file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for img_size in [4, 8, 512, 1024]:\n    folder_name = \"FFHQ_\"+str(img_size)+\"/images/\"\n    if not os.path.isdir(folder_name):\n        os.makedirs(folder_name)\n\n    data = [(file, img_size, folder_name) for file in files]\n    pool = Pool()\n    pool.starmap(resize, data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cudnn.benchmarks = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_loader(image_size):\n    transform = transforms.Compose(\n        [\n            #transforms.Resize((image_size, image_size)),\n            transforms.ToTensor(),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.Normalize(\n                [0.5 for _ in range(CHANNELS_IMG)],\n                [0.5 for _ in range(CHANNELS_IMG)],\n            ),\n        ]\n    )\n    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n    return loader, dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(\n    critic,\n    gen,\n    loader,\n    dataset,\n    step,\n    alpha,\n    opt_critic,\n    opt_gen,\n    tensorboard_step,\n    writer,\n    scaler_gen,\n    scaler_critic,\n    ema,\n):\n    loop = tqdm(loader, leave=True)\n    gen2 = Generator(\n        Z_DIM, W_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG\n    ).to(DEVICE)\n\n    for batch_idx, (real, _) in enumerate(loop):\n        real = real.to(DEVICE)\n        cur_batch_size = real.shape[0]\n\n        # Train Critic: max E[critic(real)] - E[critic(fake)] <-> min -E[critic(real)] + E[critic(fake)]\n        # which is equivalent to minimizing the negative of the expression\n        noise = torch.randn(cur_batch_size, Z_DIM).to(DEVICE)\n\n        with torch.cuda.amp.autocast():\n            fake = gen(noise, alpha, step)\n            critic_real = critic(real, alpha, step)\n            critic_fake = critic(fake.detach(), alpha, step)\n            gp = gradient_penalty(critic, real, fake, alpha, step, device=DEVICE)\n            loss_critic = (\n                -(torch.mean(critic_real) - torch.mean(critic_fake))\n                + LAMBDA_GP * gp\n                + (0.001 * torch.mean(critic_real ** 2))\n            )\n\n        opt_critic.zero_grad()\n        scaler_critic.scale(loss_critic).backward()\n        scaler_critic.step(opt_critic)\n        scaler_critic.update()\n\n        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n        with torch.cuda.amp.autocast():\n            gen_fake = critic(fake, alpha, step)\n            loss_gen = -torch.mean(gen_fake)\n\n        opt_gen.zero_grad()\n        scaler_gen.scale(loss_gen).backward()\n        scaler_gen.step(opt_gen)\n        scaler_gen.update()\n\n        # Update alpha and ensure less than 1\n        alpha += cur_batch_size / (\n            (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n        )\n        alpha = min(alpha, 1)\n\n        if batch_idx % 100 == 0:\n            ema(gen)\n            with torch.no_grad():\n                ema.copy_weights_to(gen2)\n                fixed_fakes = gen2(FIXED_NOISE, alpha, step) * 0.5 + 0.5\n            plot_to_tensorboard(\n                writer,\n                loss_critic.item(),\n                loss_gen.item(),\n                real.detach(),\n                fixed_fakes.detach(),\n                tensorboard_step,\n            )\n            tensorboard_step += 1\n\n        loop.set_postfix(\n            gp=gp.item(),\n            loss_critic=loss_critic.item(),\n        )\n\n\n    return tensorboard_step, alpha","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    # initialize gen and disc, note: discriminator should be called critic,\n    # according to WGAN paper (since it no longer outputs between [0, 1])\n    # but really who cares..\n    gen = Generator(\n        Z_DIM, W_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG\n    ).to(DEVICE)\n    critic = Discriminator(\n        IN_CHANNELS, img_channels=CHANNELS_IMG\n    ).to(DEVICE)\n    ema = EMA(gamma=0.999, save_frequency=2000)\n    # initialize optimizers and scalers for FP16 training\n    opt_gen = optim.Adam([{\"params\": [param for name, param in gen.named_parameters() if \"map\" not in name]},\n                          {\"params\": gen.map.parameters(), \"lr\": 1e-5}], lr=LEARNING_RATE, betas=(0.0, 0.99))\n    opt_critic = optim.Adam(\n        critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99)\n    )\n    scaler_critic = torch.cuda.amp.GradScaler()\n    scaler_gen = torch.cuda.amp.GradScaler()\n\n    # for tensorboard plotting\n    writer = SummaryWriter(f\"logs/gan\")\n\n    if LOAD_MODEL:\n        load_checkpoint(\n            CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE,\n        )\n        load_checkpoint(\n            CHECKPOINT_CRITIC, critic, opt_critic, LEARNING_RATE,\n        )\n\n    gen.train()\n    critic.train()\n\n    tensorboard_step = 0\n    # start at step that corresponds to img size that we set in config\n    step = int(log2(START_TRAIN_AT_IMG_SIZE / 4))\n    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n        alpha = 1e-5   # start with very low alpha\n        loader, dataset = get_loader(4 * 2 ** step)  # 4->0, 8->1, 16->2, 32->3, 64 -> 4\n        print(f\"Current image size: {4 * 2 ** step}\")\n\n        for epoch in range(num_epochs):\n            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n            tensorboard_step, alpha = train_fn(\n                critic,\n                gen,\n                loader,\n                dataset,\n                step,\n                alpha,\n                opt_critic,\n                opt_gen,\n                tensorboard_step,\n                writer,\n                scaler_gen,\n                scaler_critic,\n                ema,\n            )\n\n            if SAVE_MODEL:\n                save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n                save_checkpoint(critic, opt_critic, filename=CHECKPOINT_CRITIC)\n\n        step += 1  # progress to the next img size\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{},"execution_count":null,"outputs":[]}]}