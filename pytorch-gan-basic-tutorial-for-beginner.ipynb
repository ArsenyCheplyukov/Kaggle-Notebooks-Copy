{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Generative Adversarial Networks(GAN) - PyTorch Tutorial","metadata":{}},{"cell_type":"markdown","source":"![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FHwq72%2FbtqAY6E0wYb%2FBFRgtJWTY3Ij9BKks7vsM1%2Fimg.png)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:52:00.242873Z","iopub.execute_input":"2021-06-23T15:52:00.243235Z","iopub.status.idle":"2021-06-23T15:52:00.880086Z","shell.execute_reply.started":"2021-06-23T15:52:00.243159Z","shell.execute_reply":"2021-06-23T15:52:00.879121Z"}}},{"cell_type":"markdown","source":"This kernel is for those new to gan.\n\nAnd It was coded with pytorch, and all the code was converted into the familiar Jupyter notebook form for data analysts and machine learning engineers by referring to the gan official Python code.\n\nI hope that many Kaglers will be interested in Generative Adversarial Networks(GAN), and that it will be shared and helpful to more people. So let's get started!","metadata":{}},{"cell_type":"markdown","source":"## MAIN Reference\n1. [PyTorch-GAN | Github/eriklindernoren | Collection of PyTorch implementations of GAN](https://github.com/sw-song/PyTorch-GAN)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T13:11:22.760514Z","iopub.execute_input":"2021-06-12T13:11:22.760936Z","iopub.status.idle":"2021-06-12T13:11:22.773432Z","shell.execute_reply.started":"2021-06-12T13:11:22.760902Z","shell.execute_reply":"2021-06-12T13:11:22.771815Z"}}},{"cell_type":"markdown","source":"## Index\n```\nStep 1. Import Libraries\nStep 2. Initial Setting\nStep 3. Define Generator\nStep 4. Define Discriminator\nStep 5. Define Loss Function\nStep 6. Initialize Generator and Discriminator\nStep 7. GPU Setting\nStep 8. Configure Data Loader\nStep 9. Define Optimizers\nStep 10. Training\n```\n---","metadata":{}},{"cell_type":"markdown","source":"### Step 1. Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nimport torchvision.transforms as transforms\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.autograd import Variable \n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:04.434694Z","iopub.execute_input":"2023-03-25T12:43:04.435307Z","iopub.status.idle":"2023-03-25T12:43:05.775967Z","shell.execute_reply.started":"2023-03-25T12:43:04.435172Z","shell.execute_reply":"2023-03-25T12:43:05.774979Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Step 2. Initial setting","metadata":{}},{"cell_type":"code","source":"channels = 1 # suggested default : 1, number of image channels (gray scale)\nimg_size = 28 # suggested default : 28, size of each image dimension\nimg_shape = (channels, img_size, img_size) # (Channels, Image Size(H), Image Size(W))","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:05.777196Z","iopub.execute_input":"2023-03-25T12:43:05.777636Z","iopub.status.idle":"2023-03-25T12:43:05.781045Z","shell.execute_reply.started":"2023-03-25T12:43:05.777592Z","shell.execute_reply":"2023-03-25T12:43:05.780101Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"latent_dim = 100 # suggested default. dimensionality of the latent space","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:05.782450Z","iopub.execute_input":"2023-03-25T12:43:05.782868Z","iopub.status.idle":"2023-03-25T12:43:05.795581Z","shell.execute_reply.started":"2023-03-25T12:43:05.782838Z","shell.execute_reply":"2023-03-25T12:43:05.794702Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"cuda = True if torch.cuda.is_available() else False # GPU Setting","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:05.797021Z","iopub.execute_input":"2023-03-25T12:43:05.797451Z","iopub.status.idle":"2023-03-25T12:43:05.810044Z","shell.execute_reply.started":"2023-03-25T12:43:05.797419Z","shell.execute_reply":"2023-03-25T12:43:05.809252Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Step 3. Define Generator","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        \n        def block(input_features, output_features, normalize=True):\n            layers = [nn.Linear(input_features, output_features)]\n            if normalize: # Default\n                layers.append(nn.BatchNorm1d(output_features, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True)) # inplace=True : modify the input directly. It can slightly decrease the memory usage.\n            return layers # return list of layers\n        \n        self.model = nn.Sequential(\n            *block(latent_dim, 128, normalize=False), # Asterisk('*') in front of block means unpacking list of layers - leave only values(layers) in list\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))), # np.prod(1, 28, 28) == 1*28*28\n            nn.Tanh() # result : from -1 to 1\n        )\n\n    def forward(self, z): # z == latent vector(random input vector)\n        img = self.model(z) # (64, 100) --(model)--> (64, 784)\n        img = img.view(img.size(0), *img_shape) # img.size(0) == N(Batch Size), (N, C, H, W) == default --> (64, 1, 28, 28)\n        return img","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:05.811443Z","iopub.execute_input":"2023-03-25T12:43:05.811753Z","iopub.status.idle":"2023-03-25T12:43:05.823864Z","shell.execute_reply.started":"2023-03-25T12:43:05.811723Z","shell.execute_reply":"2023-03-25T12:43:05.822707Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"> Read More\n- [What is the difference between nn.ReLU() and nn.ReLU(inplace=True)?](https://discuss.pytorch.org/t/whats-the-difference-between-nn-relu-and-nn-relu-inplace-true/948)\n- [Tanh](https://wiki.documentfoundation.org/Documentation/Calc_Functions/TANH)\n- [Unpacking Operators in Python](https://towardsdatascience.com/unpacking-operators-in-python-306ae44cd480)","metadata":{}},{"cell_type":"markdown","source":"### Step 4. Define Discriminator","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        \n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512), # (28*28, 512)\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid() # result : from 0 to 1\n        )\n    \n    def forward(self, img):\n        img_flat = img.view(img.size(0), -1) #flatten -> from (64, 1, 28, 28) to (64, 1*28*28)\n        validity = self.model(img_flat) # Discriminate -> Real? or Fake? (64, 784) -> (64, 1)\n        return validity","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:05.825481Z","iopub.execute_input":"2023-03-25T12:43:05.825939Z","iopub.status.idle":"2023-03-25T12:43:05.836650Z","shell.execute_reply.started":"2023-03-25T12:43:05.825895Z","shell.execute_reply":"2023-03-25T12:43:05.835649Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"> Read More\n- [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)\n- [pytorch in x = x.view (x.size (0), -1) understanding](https://www.programmersought.com/article/11412923760/)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:28:58.41792Z","iopub.execute_input":"2021-06-12T15:28:58.41829Z","iopub.status.idle":"2021-06-12T15:28:58.423679Z","shell.execute_reply.started":"2021-06-12T15:28:58.418256Z","shell.execute_reply":"2021-06-12T15:28:58.422442Z"}}},{"cell_type":"markdown","source":"### Step 5. Define Loss Function","metadata":{}},{"cell_type":"code","source":"adversarial_loss = torch.nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:05.838084Z","iopub.execute_input":"2023-03-25T12:43:05.838674Z","iopub.status.idle":"2023-03-25T12:43:05.851395Z","shell.execute_reply.started":"2023-03-25T12:43:05.838605Z","shell.execute_reply":"2023-03-25T12:43:05.850703Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"> Read More\n- [BCELoss(Binary Cross Entropy Loss)](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)","metadata":{}},{"cell_type":"markdown","source":"### Step 6. Initialize generator and Discriminator","metadata":{}},{"cell_type":"code","source":"generator = Generator()\ndiscriminator = Discriminator()","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:05.855224Z","iopub.execute_input":"2023-03-25T12:43:05.855571Z","iopub.status.idle":"2023-03-25T12:43:05.915417Z","shell.execute_reply.started":"2023-03-25T12:43:05.855538Z","shell.execute_reply":"2023-03-25T12:43:05.914287Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"generator","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:05.917104Z","iopub.execute_input":"2023-03-25T12:43:05.917416Z","iopub.status.idle":"2023-03-25T12:43:05.925494Z","shell.execute_reply.started":"2023-03-25T12:43:05.917384Z","shell.execute_reply":"2023-03-25T12:43:05.924528Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Generator(\n  (model): Sequential(\n    (0): Linear(in_features=100, out_features=128, bias=True)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Linear(in_features=128, out_features=256, bias=True)\n    (3): BatchNorm1d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Linear(in_features=256, out_features=512, bias=True)\n    (6): BatchNorm1d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Linear(in_features=512, out_features=1024, bias=True)\n    (9): BatchNorm1d(1024, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n    (11): Linear(in_features=1024, out_features=784, bias=True)\n    (12): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"discriminator","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:05.926658Z","iopub.execute_input":"2023-03-25T12:43:05.926926Z","iopub.status.idle":"2023-03-25T12:43:05.937820Z","shell.execute_reply.started":"2023-03-25T12:43:05.926899Z","shell.execute_reply":"2023-03-25T12:43:05.936826Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Discriminator(\n  (model): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n    (4): Linear(in_features=256, out_features=1, bias=True)\n    (5): Sigmoid()\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Step 7. GPU Setting","metadata":{}},{"cell_type":"code","source":"if cuda:\n    generator.cuda()\n    discriminator.cuda()\n    adversarial_loss.cuda()","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:05.939120Z","iopub.execute_input":"2023-03-25T12:43:05.939752Z","iopub.status.idle":"2023-03-25T12:43:05.946333Z","shell.execute_reply.started":"2023-03-25T12:43:05.939703Z","shell.execute_reply":"2023-03-25T12:43:05.945655Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Step 8. Configure Data Loader","metadata":{}},{"cell_type":"markdown","source":"Since we use the Kaggle dataset, it is necessary to read the csv file and convert it into an image format.\n\n\nFor the code to load and convert Kaggle MNIST data, I referred to [Pytorch Dataset and DataLoader](https://www.kaggle.com/pinocookie/pytorch-dataset-and-dataloader).\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:05.947627Z","iopub.execute_input":"2023-03-25T12:43:05.948203Z","iopub.status.idle":"2023-03-25T12:43:05.956261Z","shell.execute_reply.started":"2023-03-25T12:43:05.948158Z","shell.execute_reply":"2023-03-25T12:43:05.955551Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nclass DatasetImageNet(Dataset):\n    def __init__(self,\n                 root_path=\"../input/imagenet/imagenet\",\n                 transform=transforms.Compose([\n                              transforms.ToTensor(), \n                              transforms.Normalize([0.5], [0.5]),\n                              transforms.Resize((28, 28)),\n                          ])):\n        self.root_path = root_path\n        self.transform = transform\n        \n        # get list of image file paths\n        self.img_filepaths = []\n        for subdir in os.listdir(root_path):\n            subdir_path = os.path.join(root_path, subdir)\n            print(subdir_path)\n            if os.path.isdir(subdir_path):\n                for filename in os.listdir(subdir_path):\n                    if filename.endswith(\".JPEG\"):\n                        filepath = os.path.join(subdir_path, filename)\n                        self.img_filepaths.append(filepath)\n        \n    def __len__(self):\n        return len(self.img_filepaths)\n    \n    def __getitem__(self, index):\n        # load image as numpy array (H, W, C) and resize it to (28, 28, C)\n        img_path = self.img_filepaths[index]\n        img = np.array(Image.open(img_path).resize((28, 28)))\n        \n        # apply transforms\n        if self.transform is not None:\n            img = self.transform(img)\n        \n        # convert image to tensor (C, H, W)\n        img = torch.from_numpy(np.transpose(img, (2, 0, 1)))\n        \n        # get label from folder name\n#         label = os.path.basename(os.path.dirname(img_path))\n        \n        return img, # label\n","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:05.957555Z","iopub.execute_input":"2023-03-25T12:43:05.958147Z","iopub.status.idle":"2023-03-25T12:43:05.969542Z","shell.execute_reply.started":"2023-03-25T12:43:05.958103Z","shell.execute_reply":"2023-03-25T12:43:05.968695Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"```\nclass DatasetMNIST(Dataset): # inherit abstract class - 'Dataset'\n    \n    def __init__(self, file_path, transform=None):\n        self.data = pd.read_csv(file_path)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        # load image ad ndarray type (H, W, C)\n        # be carefull for converting dtype to np.uint8 (Unsigned integer (0 to 255))\n        # in this example, We use ToTensor(), so we define the numpy array like (H, W, C)\n        \n        image = self.data.iloc[index, 1:].values.astype(np.uint8).reshape((28,28,1))\n        label = self.data.iloc[index, 0]\n        if self.transform is not None:\n            image = self.transform(image)\n        \n        return image, label\n```","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"> TEST CODE : Read CSV file","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:15:39.2396Z","iopub.execute_input":"2021-06-12T16:15:39.239996Z","iopub.status.idle":"2021-06-12T16:15:39.246142Z","shell.execute_reply.started":"2021-06-12T16:15:39.239968Z","shell.execute_reply":"2021-06-12T16:15:39.244583Z"}}},{"cell_type":"code","source":"#train = pd.read_csv('../input/digit-recognizer/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:05.971007Z","iopub.execute_input":"2023-03-25T12:43:05.971815Z","iopub.status.idle":"2023-03-25T12:43:09.798194Z","shell.execute_reply.started":"2023-03-25T12:43:05.971771Z","shell.execute_reply":"2023-03-25T12:43:09.797101Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#train","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:09.799672Z","iopub.execute_input":"2023-03-25T12:43:09.800054Z","iopub.status.idle":"2023-03-25T12:43:09.840511Z","shell.execute_reply.started":"2023-03-25T12:43:09.800016Z","shell.execute_reply":"2023-03-25T12:43:09.839485Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0          1       0       0       0       0       0       0       0       0   \n1          0       0       0       0       0       0       0       0       0   \n2          1       0       0       0       0       0       0       0       0   \n3          4       0       0       0       0       0       0       0       0   \n4          0       0       0       0       0       0       0       0       0   \n...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n41995      0       0       0       0       0       0       0       0       0   \n41996      1       0       0       0       0       0       0       0       0   \n41997      7       0       0       0       0       0       0       0       0   \n41998      6       0       0       0       0       0       0       0       0   \n41999      9       0       0       0       0       0       0       0       0   \n\n       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n0           0  ...         0         0         0         0         0   \n1           0  ...         0         0         0         0         0   \n2           0  ...         0         0         0         0         0   \n3           0  ...         0         0         0         0         0   \n4           0  ...         0         0         0         0         0   \n...       ...  ...       ...       ...       ...       ...       ...   \n41995       0  ...         0         0         0         0         0   \n41996       0  ...         0         0         0         0         0   \n41997       0  ...         0         0         0         0         0   \n41998       0  ...         0         0         0         0         0   \n41999       0  ...         0         0         0         0         0   \n\n       pixel779  pixel780  pixel781  pixel782  pixel783  \n0             0         0         0         0         0  \n1             0         0         0         0         0  \n2             0         0         0         0         0  \n3             0         0         0         0         0  \n4             0         0         0         0         0  \n...         ...       ...       ...       ...       ...  \n41995         0         0         0         0         0  \n41996         0         0         0         0         0  \n41997         0         0         0         0         0  \n41998         0         0         0         0         0  \n41999         0         0         0         0         0  \n\n[42000 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41995</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41996</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41997</th>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41998</th>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41999</th>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>42000 rows × 785 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"> TEST CODE : Construct image from csv file","metadata":{}},{"cell_type":"code","source":"# for index in range(1, 6): # N : 5 (Number of Image)\n#     temp_image = train.iloc[index, 1:].values.astype(np.uint8).reshape((28,28,1))\n#     temp_label = train.iloc[index, 0]\n#     print('Shape of Image : ',temp_image.shape)\n#     print('label : ', temp_label)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:09.841746Z","iopub.execute_input":"2023-03-25T12:43:09.842034Z","iopub.status.idle":"2023-03-25T12:43:09.853509Z","shell.execute_reply.started":"2023-03-25T12:43:09.842004Z","shell.execute_reply":"2023-03-25T12:43:09.852475Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Shape of Image :  (28, 28, 1)\nlabel :  0\nShape of Image :  (28, 28, 1)\nlabel :  1\nShape of Image :  (28, 28, 1)\nlabel :  4\nShape of Image :  (28, 28, 1)\nlabel :  0\nShape of Image :  (28, 28, 1)\nlabel :  0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> TEST CODE : transform from table data to image data with basic preprocessing","metadata":{}},{"cell_type":"code","source":"# dataset = DatasetMNIST(file_path='../input/digit-recognizer/train.csv', \n#                        transform=transforms.Compose(\n#                            [#transforms.Resize(img_size), # Resize is only for PIL Image. Not for numpy array\n#                             transforms.ToTensor(), # ToTensor() : np.array (H, W, C) -> tensor (C, H, W)\n#                             transforms.Normalize([0.5],[0.5])]\n#                        ))\ndataset = DatasetImageNet()","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:09.855057Z","iopub.execute_input":"2023-03-25T12:43:09.855654Z","iopub.status.idle":"2023-03-25T12:43:14.279845Z","shell.execute_reply.started":"2023-03-25T12:43:09.855589Z","shell.execute_reply":"2023-03-25T12:43:14.278846Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"../input/imagenet/imagenet/val\n../input/imagenet/imagenet/train\n","output_type":"stream"}]},{"cell_type":"code","source":"temp_img, _ =  dataset.__getitem__(0) # We don't need label, so _","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.281217Z","iopub.execute_input":"2023-03-25T12:43:14.281686Z","iopub.status.idle":"2023-03-25T12:43:14.452759Z","shell.execute_reply.started":"2023-03-25T12:43:14.281643Z","shell.execute_reply":"2023-03-25T12:43:14.450233Z"},"trusted":true},"execution_count":18,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-7be7d17abae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# We don't need label, so _\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-b7a2face6457>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# convert image to tensor (C, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# get label from folder name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got Tensor)"],"ename":"TypeError","evalue":"expected np.ndarray (got Tensor)","output_type":"error"}]},{"cell_type":"code","source":"temp_img.size() # before ToTensor() : (28,28,1), after : (1,28,28)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.453662Z","iopub.status.idle":"2023-03-25T12:43:14.454092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_img.max(), temp_img.min() # before Normalize([0.5],[0.5]) : 0 ~ 1, after : -1 ~ 1","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.455323Z","iopub.status.idle":"2023-03-25T12:43:14.455786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> TEST CODE : Define dataloader that can load image by batch","metadata":{"execution":{"iopub.status.busy":"2021-06-13T03:11:55.363084Z","iopub.execute_input":"2021-06-13T03:11:55.363449Z","iopub.status.idle":"2021-06-13T03:11:55.369074Z","shell.execute_reply.started":"2021-06-13T03:11:55.363409Z","shell.execute_reply":"2021-06-13T03:11:55.367783Z"}}},{"cell_type":"code","source":"batch_size = 64 # suggested default, size of the batches\ndataloader = DataLoader( # torch.utils.data.DataLoader\n    dataset,\n    batch_size=batch_size,\n    shuffle=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.456979Z","iopub.status.idle":"2023-03-25T12:43:14.457386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_images, _ = iter(dataloader).next() # We don't use label, so _\nprint('images shape on batch size = {}'.format(temp_images.size()))","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.458469Z","iopub.status.idle":"2023-03-25T12:43:14.458916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Read More\n1. [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)\n    - .ToTensor | Convert a PIL Image or numpy.ndarray to tensor. This transform does not support torchscript.\n    - .ToTensor | Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8","metadata":{}},{"cell_type":"markdown","source":"### Step 9. Define optimizers","metadata":{}},{"cell_type":"code","source":"# suggested default - beta parameters (decay of first order momentum of gradients)\nb1 = 0.5\nb2 = 0.999\n\n# suggested default - learning rate\nlr = 0.0002 ","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.459931Z","iopub.status.idle":"2023-03-25T12:43:14.460327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1,b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1,b2))","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.461392Z","iopub.status.idle":"2023-03-25T12:43:14.461806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 10. Training","metadata":{}},{"cell_type":"markdown","source":"![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FpMdme%2FbtqA1ArBCOy%2FqqGg7IvV0hpqVkvBuEFpJK%2Fimg.png)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:52:41.513245Z","iopub.execute_input":"2021-06-23T15:52:41.513601Z","iopub.status.idle":"2021-06-23T15:52:42.137653Z","shell.execute_reply.started":"2021-06-23T15:52:41.513557Z","shell.execute_reply":"2021-06-23T15:52:42.136721Z"}}},{"cell_type":"code","source":"Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.463086Z","iopub.status.idle":"2023-03-25T12:43:14.463544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.464532Z","iopub.status.idle":"2023-03-25T12:43:14.464964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize result\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.465900Z","iopub.status.idle":"2023-03-25T12:43:14.466353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 250 # suggested default = 200\nfor epoch in range(n_epochs):\n    for i, (imgs, _) in enumerate(tqdm(dataloader)): # This code(enumerate) is dealt with once more in the *TEST_CODE below.\n                                                     # Used 'tqdm' for showing progress \n        \n        # Adversarial ground truths (For more detail, refer *Read_More below)\n        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False) # imgs.size(0) == batch_size(1 batch) == 64, *TEST_CODE\n        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False) # And Variable is for caclulate gradient. In fact, you can use it, but you don't have to. \n                                                                                # requires_grad=False is default in tensor type. *Read_More\n        \n        # Configure input\n        real_imgs = imgs.type(Tensor) # As mentioned, it is no longer necessary to wrap the tensor in a Variable.\n      # real_imgs = Variable(imgs.type(Tensor)) # requires_grad=False, Default! It's same.\n    \n# ------------\n# Train Generator\n# ------------\n        optimizer_G.zero_grad()\n        \n        # sample noise 'z' as generator input\n        z = Tensor(np.random.normal(0, 1, (imgs.shape[0],latent_dim))) # Random sampling Tensor(batch_size, latent_dim) of Gaussian distribution\n        # z.shape == torch.Size([64, 100])\n        \n        # Generate a batch of images\n        gen_imgs = generator(z)\n        # gen_imgs.shape == torch.Size([64, 1, 28, 28])\n        \n        # Loss measures generator's ability to fool the discriminator\n        g_loss = adversarial_loss(discriminator(gen_imgs), valid) # torch.nn.BCELoss() compare result(64x1) and valid(64x1, filled with 1)\n        \n        g_loss.backward()\n        optimizer_G.step()\n        \n# ------------\n# Train Discriminator\n# ------------\n        optimizer_D.zero_grad()\n        \n        # Measure discriminator's ability to classify real from generated samples\n        real_loss = adversarial_loss(discriminator(real_imgs), valid) # torch.nn.BCELoss() compare result(64x1) and valid(64x1, filled with 1)\n        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake) # We are learning the discriminator now. So have to use detach() \n                                                                             \n        d_loss = (real_loss + fake_loss) / 2\n        \n        d_loss.backward()# If didn't use detach() for gen_imgs, all weights of the generator will be calculated with backward(). \n        optimizer_D.step()\n         \n    \n\n# ------------\n# Real Time Visualization (While Training)\n# ------------\n        \n        sample_z_in_train = Tensor(np.random.normal(0, 1, (imgs.shape[0],latent_dim)))\n        # z.shape == torch.Size([64, 100])\n        sample_gen_imgs_in_train = generator(sample_z_in_train).detach().cpu()\n        # gen_imgs.shape == torch.Size([64, 1, 28, 28])\n        \n        if ((i+1) % 200) == 0: # show while batch - 200/657, 400/657, 600/657\n            nrow=1\n            ncols=5\n            fig, axes = plt.subplots(nrows=nrow,ncols=ncols, figsize=(8,2))\n            plt.suptitle('EPOCH : {} | BATCH(ITERATION) : {}'.format(epoch+1, i+1))\n            for ncol in range(ncols):\n                axes[ncol].imshow(sample_gen_imgs_in_train.permute(0,2,3,1)[ncol], cmap='gray')\n                axes[ncol].axis('off')\n            plt.show()\n    print(\n        \"[Epoch: %d/%d] [Batch: %d/%d] [D loss: %f] [G loss: %f]\"\n        % (epoch+1, n_epochs, i+1, len(dataloader), d_loss.item(), g_loss.item())\n    )","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.467258Z","iopub.status.idle":"2023-03-25T12:43:14.467674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> TEST CODE : enumerate >> [docs.python.org/enumerate](https://docs.python.org/3/library/functions.html#enumerate)","metadata":{}},{"cell_type":"code","source":"# result of enumerate\ncount = 1\nfor i, (imgs,label) in enumerate(dataloader):\n    print('Shape of Batch Images : \\n', imgs.shape)\n    print('Labels (1~64) : \\n', label)\n    print('-'*100)\n    if count == 5:\n        break\n    else:\n        count += 1","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.468435Z","iopub.status.idle":"2023-03-25T12:43:14.469041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> TEST CODE : \n- Read CSV file\n- Tensor(imgs.size(0),1)\n- Tensor(imgs.size(0),1).fill_(1.0)","metadata":{}},{"cell_type":"code","source":"Tensor(10,1) # Just 10 for a quick look. \n             # We dealt with Tensor(imgs.size(0),1) above. Tensor(64,1) <-- len(H) == batch_size == 64, len(W) == 1","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.469878Z","iopub.status.idle":"2023-03-25T12:43:14.470278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tensor(10,1).fill_(1.0) # _ means inplace, fill the Tensor with 1.0","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.471029Z","iopub.status.idle":"2023-03-25T12:43:14.471425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> TEST CODE : Variable and requires_grad","metadata":{}},{"cell_type":"code","source":"sample_img = iter(dataloader).next()[0]","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.472207Z","iopub.status.idle":"2023-03-25T12:43:14.472587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_img.shape, sample_img.dtype","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.473463Z","iopub.status.idle":"2023-03-25T12:43:14.473868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_img.requires_grad # default : False ","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.475184Z","iopub.status.idle":"2023-03-25T12:43:14.475592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Variable(sample_img).requires_grad # exactly same","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.476398Z","iopub.status.idle":"2023-03-25T12:43:14.476813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_img.requires_grad_(True) # set requires_grad to True, _ means inplace","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.477629Z","iopub.status.idle":"2023-03-25T12:43:14.478029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_img.requires_grad # Yes. requires_grad changed correctly. you don't need wrapping tensor with Variable. ","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.478794Z","iopub.status.idle":"2023-03-25T12:43:14.479196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, it can be confusing here. Obviously, the derivative is computed through the loss, but not a single `requires_grad` is set to `true`.\n\nRemember, It's different from `model's parameter`. Internally, the parameters of each Module are stored in Tensors with `requires_grad=True` !","metadata":{}},{"cell_type":"markdown","source":"> TEST CODE : Ramdom Sampling (H,W) from a normal(Gaussian) distribution ","metadata":{}},{"cell_type":"code","source":"np.random.normal(0,1,(64,100))","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.479937Z","iopub.status.idle":"2023-03-25T12:43:14.480472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.normal(0,1,(64,100)).shape","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.481360Z","iopub.status.idle":"2023-03-25T12:43:14.481776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> TEST CODE : shape of z and gen_imgs and discriminator(gen_imgs)","metadata":{}},{"cell_type":"code","source":"# latent vector\nsample_z = Tensor(np.random.normal(0, 1, (64,100)))\nsample_z.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.482648Z","iopub.status.idle":"2023-03-25T12:43:14.483050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generated images\nsample_gen_imgs = generator(sample_z)\nsample_gen_imgs.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.483912Z","iopub.status.idle":"2023-03-25T12:43:14.484323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# discrimination result\nsample_discrim_result = discriminator(sample_gen_imgs)\nsample_discrim_result.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.485193Z","iopub.status.idle":"2023-03-25T12:43:14.485585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> TEST CODE : g_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-13T03:32:26.913011Z","iopub.execute_input":"2021-06-13T03:32:26.913329Z","iopub.status.idle":"2021-06-13T03:32:26.917897Z","shell.execute_reply.started":"2021-06-13T03:32:26.913302Z","shell.execute_reply":"2021-06-13T03:32:26.917117Z"}}},{"cell_type":"code","source":"adversarial_loss","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.486480Z","iopub.status.idle":"2023-03-25T12:43:14.486888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_valid.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.487717Z","iopub.status.idle":"2023-03-25T12:43:14.488116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_valid = Tensor(64,1).fill_(1.0)\nsample_g_loss = adversarial_loss(sample_discrim_result, sample_valid)\nsample_g_loss","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.488881Z","iopub.status.idle":"2023-03-25T12:43:14.489280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> TEST CODE : detach()\n- refer : [What is PyTorch '.detach()' method? - theroyakash, 2020-11](https://dev.to/theroyakash/what-is-pytorch-detach-method-15oo)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T08:28:06.970211Z","iopub.execute_input":"2021-06-14T08:28:06.971551Z","iopub.status.idle":"2021-06-14T08:28:06.994767Z","shell.execute_reply.started":"2021-06-14T08:28:06.971123Z","shell.execute_reply":"2021-06-14T08:28:06.992784Z"}}},{"cell_type":"code","source":"!pip install torchviz\nimport torchviz","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.490075Z","iopub.status.idle":"2023-03-25T12:43:14.490484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = torch.ones((28,28), dtype=torch.float32, requires_grad=True)\nsquare_X = X**2\ncubic_X = X**3\n\nresult = (square_X+cubic_X).sum()\n\ntorchviz.make_dot(result)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.491595Z","iopub.status.idle":"2023-03-25T12:43:14.492010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = torch.ones((28,28), dtype=torch.float32, requires_grad=True)\nsquare_X = X**2\ncubic_X = X.detach()**3\n\nresult = (square_X+cubic_X).sum()\n\ntorchviz.make_dot(result)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T12:43:14.492808Z","iopub.status.idle":"2023-03-25T12:43:14.493206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Read More\n- [What is Ground Truth? - Definition from Techpedia](https://www.techopedia.com/definition/32514/ground-truth)\n- [tqdm. A Fast, Extensible Progress Bar for Python and CLI](https://github.com/tqdm/tqdm)\n- [Variables are no longer necessary to use autograd with tensors](https://pytorch.org/docs/stable/autograd.html#variable-deprecated)\n- [np.random.normal(loc=0.0, scale=1.0, size=None)](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html)\n\n","metadata":{}},{"cell_type":"markdown","source":"--------------","metadata":{}},{"cell_type":"markdown","source":"\nSo far, we have completed the basic model of gan and tested the model through mnist data.\n\nI plan to implement and test more various gan models from the very basics through this series, so let's study together!\n\nThank you and Enjoy your Kaggle! :)","metadata":{}}]}