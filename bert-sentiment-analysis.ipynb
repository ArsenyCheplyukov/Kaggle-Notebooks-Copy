{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Определение эмоциональной окраски твитов с помощью BERT**","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt -q\nimport sys; sys.path.append('./stepik-dl-nlp')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-26T10:59:21.063896Z","iopub.execute_input":"2021-07-26T10:59:21.064323Z","iopub.status.idle":"2021-07-26T11:01:12.807757Z","shell.execute_reply.started":"2021-07-26T10:59:21.064227Z","shell.execute_reply":"2021-07-26T11:01:12.806728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Установка библиотек**","metadata":{}},{"cell_type":"code","source":"!pip install pytorch-transformers","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:01:12.809612Z","iopub.execute_input":"2021-07-26T11:01:12.809961Z","iopub.status.idle":"2021-07-26T11:01:20.562042Z","shell.execute_reply.started":"2021-07-26T11:01:12.809924Z","shell.execute_reply":"2021-07-26T11:01:20.560996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_transformers import BertTokenizer, BertConfig\nfrom pytorch_transformers import AdamW, BertForSequenceClassification\nimport tqdm \nfrom tqdm import trange\nimport pandas as pd\nimport io\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:41:09.969841Z","iopub.execute_input":"2021-07-26T11:41:09.970163Z","iopub.status.idle":"2021-07-26T11:41:09.975602Z","shell.execute_reply.started":"2021-07-26T11:41:09.970132Z","shell.execute_reply":"2021-07-26T11:41:09.974693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device == 'cpu':\n    print('cpu')\nelse:\n    n_gpu = torch.cuda.device_count()\n    print(torch.cuda.get_device_name(0))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:01:26.293268Z","iopub.execute_input":"2021-07-26T11:01:26.293594Z","iopub.status.idle":"2021-07-26T11:01:26.345002Z","shell.execute_reply.started":"2021-07-26T11:01:26.293555Z","shell.execute_reply":"2021-07-26T11:01:26.342560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Загрузка данных**\n#### **Мы выбрали необычный датасет с разметкой сентимента русскоязычных твитов (подробнее про него в [статье](http://www.swsys.ru/index.php?page=article&id=3962&lang=)). В корпусе, который мы использовали 114,911 положительных и 111,923 отрицательных записей. Загрузить его можно [тут](https://study.mokoron.com/).**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\npos_texts = pd.read_csv('./stepik-dl-nlp/datasets/bert_sentiment_analysis/positive.csv', encoding='utf8', sep=';', header=None)\nneg_texts = pd.read_csv('./stepik-dl-nlp/datasets/bert_sentiment_analysis/negative.csv', encoding='utf8', sep=';', header=None)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:01:26.346444Z","iopub.execute_input":"2021-07-26T11:01:26.346886Z","iopub.status.idle":"2021-07-26T11:01:27.351567Z","shell.execute_reply.started":"2021-07-26T11:01:26.346845Z","shell.execute_reply":"2021-07-26T11:01:27.350747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_texts.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:01:27.352914Z","iopub.execute_input":"2021-07-26T11:01:27.353246Z","iopub.status.idle":"2021-07-26T11:01:27.384126Z","shell.execute_reply.started":"2021-07-26T11:01:27.353211Z","shell.execute_reply":"2021-07-26T11:01:27.383175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = np.concatenate([pos_texts[3].values, neg_texts[3].values])\n\nsentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\nlabels = [[1] for _ in range(pos_texts.shape[0])] + [[0] for _ in range(neg_texts.shape[0])]","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:01:27.385492Z","iopub.execute_input":"2021-07-26T11:01:27.385858Z","iopub.status.idle":"2021-07-26T11:01:27.882049Z","shell.execute_reply.started":"2021-07-26T11:01:27.385822Z","shell.execute_reply":"2021-07-26T11:01:27.881127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(sentences) == len(labels) == pos_texts.shape[0] + neg_texts.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:01:27.884896Z","iopub.execute_input":"2021-07-26T11:01:27.885270Z","iopub.status.idle":"2021-07-26T11:01:27.890781Z","shell.execute_reply.started":"2021-07-26T11:01:27.885234Z","shell.execute_reply":"2021-07-26T11:01:27.889767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sentences[1000])","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:01:27.892972Z","iopub.execute_input":"2021-07-26T11:01:27.893628Z","iopub.status.idle":"2021-07-26T11:01:27.901244Z","shell.execute_reply.started":"2021-07-26T11:01:27.893591Z","shell.execute_reply":"2021-07-26T11:01:27.900352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_sentences, test_sentences, train_gt, test_gt = train_test_split(sentences, labels, test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:01:27.902684Z","iopub.execute_input":"2021-07-26T11:01:27.903137Z","iopub.status.idle":"2021-07-26T11:01:28.118778Z","shell.execute_reply.started":"2021-07-26T11:01:27.903098Z","shell.execute_reply":"2021-07-26T11:01:28.117726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_gt), len(test_gt))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:01:28.124570Z","iopub.execute_input":"2021-07-26T11:01:28.126806Z","iopub.status.idle":"2021-07-26T11:01:28.136706Z","shell.execute_reply.started":"2021-07-26T11:01:28.126760Z","shell.execute_reply":"2021-07-26T11:01:28.135484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Inputs**","metadata":{}},{"cell_type":"code","source":"from pytorch_transformers import BertTokenizer, BertConfig\n\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\ntokenized_texts = [tokenizer.tokenize(sent) for sent in train_sentences]\nprint (tokenized_texts[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:01:28.141940Z","iopub.execute_input":"2021-07-26T11:01:28.144253Z","iopub.status.idle":"2021-07-26T11:03:10.804571Z","shell.execute_reply.started":"2021-07-26T11:01:28.144210Z","shell.execute_reply":"2021-07-26T11:03:10.802871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **BERTу нужно предоставить специальный формат входных данных.**\n\n* input ids: последовательность чисел, отождествляющих каждый токен с его номером в словаре.\n* labels: вектор из нулей и единиц. В нашем случае нули обозначают негативную эмоциональную окраску, единицы - положительную.\n* segment mask: (необязательно) последовательность нулей и единиц, которая показывает, состоит ли входной текст из одного или двух предложений. Для случая одного предложения получится вектор из одних нулей. Для двух:\n* attention mask: (необязательно) последовательность нулей и единиц, где единицы обозначают токены предложения, нули - паддинг.","metadata":{}},{"cell_type":"code","source":"input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\ninput_ids = pad_sequences(\n    input_ids,\n    maxlen=100,\n    dtype=\"long\",\n    truncating=\"post\",\n    padding=\"post\"\n)\nattention_masks = [[float(i>0) for i in seq] for seq in input_ids]","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:03:10.805998Z","iopub.execute_input":"2021-07-26T11:03:10.806335Z","iopub.status.idle":"2021-07-26T11:03:39.060585Z","shell.execute_reply.started":"2021-07-26T11:03:10.806297Z","shell.execute_reply":"2021-07-26T11:03:39.059636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n    input_ids, train_gt, \n    random_state=42,\n    test_size=0.1\n)\n\ntrain_masks, validation_masks, _, _ = train_test_split(\n    attention_masks,\n    input_ids,\n    random_state=42,\n    test_size=0.1\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:03:39.061963Z","iopub.execute_input":"2021-07-26T11:03:39.062301Z","iopub.status.idle":"2021-07-26T11:03:39.297934Z","shell.execute_reply.started":"2021-07-26T11:03:39.062266Z","shell.execute_reply":"2021-07-26T11:03:39.297051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_inputs = torch.tensor(train_inputs)[::10]\ntrain_labels = torch.tensor(train_labels)[::10]\ntrain_masks = torch.tensor(train_masks)[::10]","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:44:28.836291Z","iopub.execute_input":"2021-07-26T11:44:28.836620Z","iopub.status.idle":"2021-07-26T11:44:28.875118Z","shell.execute_reply.started":"2021-07-26T11:44:28.836589Z","shell.execute_reply":"2021-07-26T11:44:28.874279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_inputs = torch.tensor(validation_inputs)\nvalidation_labels = torch.tensor(validation_labels)\nvalidation_masks = torch.tensor(validation_masks)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:44:32.907602Z","iopub.execute_input":"2021-07-26T11:44:32.907979Z","iopub.status.idle":"2021-07-26T11:44:32.916889Z","shell.execute_reply.started":"2021-07-26T11:44:32.907949Z","shell.execute_reply":"2021-07-26T11:44:32.915827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:44:44.208230Z","iopub.execute_input":"2021-07-26T11:44:44.208655Z","iopub.status.idle":"2021-07-26T11:44:44.225510Z","shell.execute_reply.started":"2021-07-26T11:44:44.208618Z","shell.execute_reply":"2021-07-26T11:44:44.224516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_dataloader = DataLoader(\n    train_data,\n    sampler=RandomSampler(train_data),\n    batch_size=32\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:44:44.227330Z","iopub.execute_input":"2021-07-26T11:44:44.229509Z","iopub.status.idle":"2021-07-26T11:44:44.237822Z","shell.execute_reply.started":"2021-07-26T11:44:44.229469Z","shell.execute_reply":"2021-07-26T11:44:44.236852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_dataloader = DataLoader(\n    validation_data,\n    sampler=SequentialSampler(validation_data),\n    batch_size=32\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:44:44.242622Z","iopub.execute_input":"2021-07-26T11:44:44.244757Z","iopub.status.idle":"2021-07-26T11:44:44.254097Z","shell.execute_reply.started":"2021-07-26T11:44:44.244680Z","shell.execute_reply":"2021-07-26T11:44:44.253275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Обучение модели**\n#### **Загружаем [BertForSequenceClassification](https://github.com/huggingface/transformers/blob/master/pytorch_pretrained_bert/modeling.py#L1129):**","metadata":{}},{"cell_type":"code","source":"from pytorch_transformers import AdamW, BertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:44:44.255930Z","iopub.execute_input":"2021-07-26T11:44:44.257492Z","iopub.status.idle":"2021-07-26T11:44:44.264454Z","shell.execute_reply.started":"2021-07-26T11:44:44.257455Z","shell.execute_reply":"2021-07-26T11:44:44.263680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Аналогичные модели есть и для других задач:**","metadata":{}},{"cell_type":"code","source":"from pytorch_transformers import BertForQuestionAnswering, BertForTokenClassification","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:44:44.265941Z","iopub.execute_input":"2021-07-26T11:44:44.267206Z","iopub.status.idle":"2021-07-26T11:44:44.274370Z","shell.execute_reply.started":"2021-07-26T11:44:44.267151Z","shell.execute_reply":"2021-07-26T11:44:44.273508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\nmodel.cuda()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:46:36.618183Z","iopub.execute_input":"2021-07-26T11:46:36.618521Z","iopub.status.idle":"2021-07-26T11:46:40.869778Z","shell.execute_reply.started":"2021-07-26T11:46:36.618489Z","shell.execute_reply":"2021-07-26T11:46:40.868945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n\noptimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:46:46.318907Z","iopub.execute_input":"2021-07-26T11:46:46.319268Z","iopub.status.idle":"2021-07-26T11:46:46.336201Z","shell.execute_reply.started":"2021-07-26T11:46:46.319236Z","shell.execute_reply":"2021-07-26T11:46:46.335197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\n\n# Будем сохранять loss во время обучения\n# и рисовать график в режиме реального времени\ntrain_loss_set = []\ntrain_loss = 0\n\n\n# Обучение\n# Переводим модель в training mode\nmodel.train()\n\n\nfor step, batch in tqdm.tqdm(enumerate(train_dataloader)):\n    # добавляем батч для вычисления на GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Распаковываем данные из dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    # если не сделать .zero_grad(), градиенты будут накапливаться\n    optimizer.zero_grad()\n    \n    # Forward pass\n    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n\n    train_loss_set.append(loss[0].item())  \n    \n    # Backward pass\n    loss[0].backward()\n    \n    # Обновляем параметры и делаем шаг используя посчитанные градиенты\n    optimizer.step()\n\n    # Обновляем loss\n    train_loss += loss[0].item()\n    \n    # Рисуем график\n    clear_output(True)\n    plt.plot(train_loss_set)\n    plt.title(\"Training loss\")\n    plt.xlabel(\"Batch\")\n    plt.ylabel(\"Loss\")\n    plt.show()\n    \nprint(\"Loss на обучающей выборке: {0:.5f}\".format(train_loss / len(train_dataloader)))\n\n\n# Валидация\n# Переводим модель в evaluation mode\nmodel.eval()\n\nvalid_preds, valid_labels = [], []\n\nfor batch in validation_dataloader:   \n    # добавляем батч для вычисления на GPU\n    batch = tuple(t.to(device) for t in batch)\n    \n    # Распаковываем данные из dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    # При использовании .no_grad() модель не будет считать и хранить градиенты.\n    # Это ускорит процесс предсказания меток для валидационных данных.\n    with torch.no_grad():\n        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n    # Перемещаем logits и метки классов на CPU для дальнейшей работы\n    logits = logits[0].detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    \n    batch_preds = np.argmax(logits, axis=1)\n    batch_labels = np.concatenate(label_ids)     \n    valid_preds.extend(batch_preds)\n    valid_labels.extend(batch_labels)\n\nprint(\"Процент правильных предсказаний на валидационной выборке: {0:.2f}%\".format(\n    accuracy_score(valid_labels, valid_preds) * 100\n))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:46:46.338132Z","iopub.execute_input":"2021-07-26T11:46:46.338529Z","iopub.status.idle":"2021-07-26T11:51:21.094515Z","shell.execute_reply.started":"2021-07-26T11:46:46.338489Z","shell.execute_reply":"2021-07-26T11:51:21.093500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Процент правильных предсказаний на валидационной выборке: {0:.2f}%\".format(\n    accuracy_score(valid_labels, valid_preds) * 100\n))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:55:46.138017Z","iopub.status.idle":"2021-07-26T11:55:46.138583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Оценка качества на отложенной выборке**","metadata":{}},{"cell_type":"code","source":"tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\ninput_ids = pad_sequences(\n    input_ids,\n    maxlen=100,\n    dtype=\"long\",\n    truncating=\"post\",\n    padding=\"post\"\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T11:55:48.493903Z","iopub.execute_input":"2021-07-26T11:55:48.494272Z","iopub.status.idle":"2021-07-26T11:56:36.783245Z","shell.execute_reply.started":"2021-07-26T11:55:48.494239Z","shell.execute_reply":"2021-07-26T11:56:36.782391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n\nprediction_inputs = torch.tensor(input_ids)\nprediction_masks = torch.tensor(attention_masks)\nprediction_labels = torch.tensor(test_gt)\n\nprediction_data = TensorDataset(\n    prediction_inputs,\n    prediction_masks,\n    prediction_labels\n)\n\nprediction_dataloader = DataLoader(\n    prediction_data, \n    sampler=SequentialSampler(prediction_data),\n    batch_size=32\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T12:01:02.229669Z","iopub.execute_input":"2021-07-26T12:01:02.230077Z","iopub.status.idle":"2021-07-26T12:01:10.476527Z","shell.execute_reply.started":"2021-07-26T12:01:02.230043Z","shell.execute_reply":"2021-07-26T12:01:10.475596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ntest_preds, test_labels = [], []\n\nfor batch in prediction_dataloader:\n    # добавляем батч для вычисления на GPU\n    batch = tuple(t.to(device) for t in batch)\n    \n    # Распаковываем данные из dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    # При использовании .no_grad() модель не будет считать и хранить градиенты.\n    # Это ускорит процесс предсказания меток для тестовых данных.\n    with torch.no_grad():\n        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n    # Перемещаем logits и метки классов на CPU для дальнейшей работы\n    logits = logits[0].detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    # Сохраняем предсказанные классы и ground truth\n    batch_preds = np.argmax(logits, axis=1)\n    batch_labels = np.concatenate(label_ids)  \n    test_preds.extend(batch_preds)\n    test_labels.extend(batch_labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T12:01:15.347495Z","iopub.execute_input":"2021-07-26T12:01:15.347926Z","iopub.status.idle":"2021-07-26T12:05:01.282520Z","shell.execute_reply.started":"2021-07-26T12:01:15.347892Z","shell.execute_reply":"2021-07-26T12:05:01.281531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_score = accuracy_score(test_labels, test_preds)\nprint('Процент правильных предсказаний на отложенной выборке составил: {0:.2f}%'.format(\n    acc_score*100\n))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T12:05:01.284125Z","iopub.execute_input":"2021-07-26T12:05:01.284504Z","iopub.status.idle":"2021-07-26T12:05:01.364157Z","shell.execute_reply.started":"2021-07-26T12:05:01.284448Z","shell.execute_reply":"2021-07-26T12:05:01.363176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Неправильных предсказаний: {0}/{1}'.format(\n    sum(test_labels != test_preds),\n    len(test_labels)\n))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T12:05:01.366289Z","iopub.execute_input":"2021-07-26T12:05:01.366710Z","iopub.status.idle":"2021-07-26T12:05:01.394115Z","shell.execute_reply.started":"2021-07-26T12:05:01.366667Z","shell.execute_reply":"2021-07-26T12:05:01.392541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Домашнее задание**\n#### **Скачайте датасет с отзывами на фильмы. Например, используйте датасет ../input**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndataset = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T12:05:01.395498Z","iopub.status.idle":"2021-07-26T12:05:01.396132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T12:05:01.397453Z","iopub.status.idle":"2021-07-26T12:05:01.398178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Используйте для дообучения BERT датасет IMDB.**\n\n#### **Ответьте на вопросы:**\n\n* **удалось ли достичь такого же accuracy (98\\%) при использовании IMDB датасета?**\n* **удалось ли получить хорошее качество классификации всего за одну эпоху?\nподумайте, в чем может быть причина различий в дообучении одной и той же модели на разных датасетах**\n* **Внимательно изучите датасет с русскими твитами. В чем его особенности? Нет ли явных паттернов или ключевых слов, которые однозначно определяют сентимент твита?**\n* **Попробуйте удалить пунктуацию из датасета с русскими твитами и перезапустите дообучение модели. Изменилось ли итоговое качество работы модели? Почему?**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}