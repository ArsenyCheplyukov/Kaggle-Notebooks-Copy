{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers[torch]","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:30:50.683911Z","iopub.execute_input":"2021-08-06T09:30:50.684300Z","iopub.status.idle":"2021-08-06T09:30:56.955845Z","shell.execute_reply.started":"2021-08-06T09:30:50.684268Z","shell.execute_reply":"2021-08-06T09:30:56.954674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:31:00.211494Z","iopub.execute_input":"2021-08-06T09:31:00.211845Z","iopub.status.idle":"2021-08-06T09:31:01.960744Z","shell.execute_reply.started":"2021-08-06T09:31:00.211808Z","shell.execute_reply":"2021-08-06T09:31:01.959567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")","metadata":{"execution":{"iopub.status.busy":"2021-08-06T10:14:05.108252Z","iopub.execute_input":"2021-08-06T10:14:05.108596Z","iopub.status.idle":"2021-08-06T10:14:23.470418Z","shell.execute_reply.started":"2021-08-06T10:14:05.108563Z","shell.execute_reply":"2021-08-06T10:14:23.469357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from time import time\n# Let's chat for 5 lines\npassed = 0\nfor step in range(5):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n    strt = time()\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(\n        bot_input_ids,\n        max_length=1000,\n        #num_beams=5, # 3\n        #do_sample=True,\n        top_k=0, #100\n        temperature=0.75,\n        top_p=0.95,\n        early_stopping=True,\n        pad_token_id=tokenizer.eos_token_id)\n    \n    # pretty print last ouput tokens from bot\n    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n    passed += time() - strt\nprint(f\"TOTAL EXECUTION TIME IS: {passed}s\")","metadata":{"execution":{"iopub.status.busy":"2021-08-06T10:23:52.192080Z","iopub.execute_input":"2021-08-06T10:23:52.192436Z","iopub.status.idle":"2021-08-06T10:24:32.538476Z","shell.execute_reply.started":"2021-08-06T10:23:52.192406Z","shell.execute_reply":"2021-08-06T10:24:32.537283Z"},"trusted":true},"execution_count":null,"outputs":[]}]}