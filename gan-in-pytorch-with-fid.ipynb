{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\nThe main idea behind this kernel is to implement DCGAN in Pytorch with some improvement techniques and implement Fretchet Inception Distance along with it. GANs are one of my favorite neural networks and one of the biggest pain points was assesing it's performance as precisely as we can do for other neural networks. Knowing when to stop the training(reaching Nash equilbirium) or comparing two GAN models was never straightforward. This drawback is overcomed by using Fretchet Inception Distance which is also considered superior to it's predecessor Inception Score.  \n\nFID was introduced in the paper [GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium\n](https://arxiv.org/abs/1706.08500)\n\nI have picked up the underlying DCGAN implementation from this [Pytorch tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) and have iteratively improved upon it by some hacks discussed later and compared performance between experiments using FID which I'll also discuss later in the kernel.","metadata":{"_uuid":"b0a9636b-b648-41b8-ba82-c5420af70ac2","_cell_guid":"9c110ec6-42ee-41c1-910a-07bb3f089f8a","trusted":true}},{"cell_type":"code","source":"# cifar10-python\n\nfrom __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision.models as models\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nfrom scipy import linalg\nfrom torch.nn.functional import adaptive_avg_pool2d\n\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\nimport os\n# print(os.listdir(\"../input\"))\n\nimport time\n\nfrom tqdm import tqdm","metadata":{"_uuid":"f7601d03-a483-410b-b136-e2fd99bb0c9a","_cell_guid":"f6180422-a419-47ed-acbc-93c9b0722cc2","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-05-07T15:59:35.637225Z","iopub.execute_input":"2023-05-07T15:59:35.637501Z","iopub.status.idle":"2023-05-07T15:59:36.795184Z","shell.execute_reply.started":"2023-05-07T15:59:35.637446Z","shell.execute_reply":"2023-05-07T15:59:36.794438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generative Adversarial Networks\nA quick recap in case you don't know about GANs.\n\n> * GANs are a class of Unsupervised Learning Algorithms that implement deep neural networks and are comprised of two parts(networks), pitting one against the other (thus the “adversarial”). These two parts are called the Generator and the Discriminator.\n* The **Generator** takes the role of a forger and tries to create real images(in our case) from random noise.While the **Discriminator** takes the role of an evaluator and tries to distinguish real images from fake ones.\n* The generator tries to maximize the probability of fooling the Discriminator by making the images(for example) more close to real in each step thereby making the Discriminator classify them as real.And the discriminator guides the generator to produce more realistic images , by classifying it's images as fake.\n* This min-max game is continued until a [Nash equilibrium](https://www.google.com/search?q=nash+equilibrium&rlz=1C1SQJL_enIN853IN853&oq=nash+&aqs=chrome.1.69i57j0l5j69i61j69i60.5649j0j9&sourceid=chrome&ie=UTF-8) is reached.\n* **DCGAN** is a variant of vanilla GAN in which Deep Convolutional layer are used in both Generator and Discriminator instead of using Dense Layers.\n* There are some hacks that I experiment with, mentioned in the github repo [ganhacks](https://github.com/soumith/ganhacks)\n\nSo let's set the seeds and some hyperparameters and get started.","metadata":{"_uuid":"e7613a30-7540-40a4-acfd-246e54d46490","_cell_guid":"071615f4-7c98-455c-a27f-ba371b6f2aea","trusted":true}},{"cell_type":"code","source":"SEED=42\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n# Batch size during training\nbatch_size = 128\n# Spatial size of training images. All images will be resized to this size using a transformer.\nimage_size = 64\n# Number of channels in the training images. For color images this is 3\nnc = 3\n# Size of z latent vector (i.e. size of generator input)\nnz = 100\n# Size of feature maps in generator\nngf = 64\n# Size of feature maps in discriminator\nndf = 64\n# Number of training epochs\nnum_epochs = 70\n# different Learning rate for optimizers\ng_lr = 0.0001\nd_lr = 0.0004\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.5\nngpu=1","metadata":{"_uuid":"eb623940-bba4-4e87-9531-48172ef08df5","_cell_guid":"43d4347b-a61f-4820-8650-d0136cca00a5","execution":{"iopub.status.busy":"2023-05-07T15:59:36.796319Z","iopub.execute_input":"2023-05-07T15:59:36.796585Z","iopub.status.idle":"2023-05-07T15:59:36.805912Z","shell.execute_reply.started":"2023-05-07T15:59:36.796539Z","shell.execute_reply":"2023-05-07T15:59:36.804717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torchvision.datasets import ImageFolder\n\nclass MyDataModule(Dataset):\n    def __init__(self, root_dir, transform):\n        self.transform = transform\n        self.dataset = ImageFolder(root=root_dir, transform=transform)\n\n    def __getitem__(self, idx):\n        image, label = self.dataset[idx]\n        return image, label\n\n    def __len__(self):\n        return len(self.dataset)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T15:59:36.807895Z","iopub.execute_input":"2023-05-07T15:59:36.808477Z","iopub.status.idle":"2023-05-07T15:59:36.814212Z","shell.execute_reply.started":"2023-05-07T15:59:36.808424Z","shell.execute_reply":"2023-05-07T15:59:36.813643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'll be using CIFAR-10 dataset, that'll be fed to the Discriminator as real images.Let's load that.","metadata":{"_uuid":"a65f3933-65f3-4281-9d3c-3e50619df37a","_cell_guid":"1c04aa71-4906-4d49-ac03-aa3d611eaa19","trusted":true}},{"cell_type":"code","source":"#normalizing input between -1 and 1\ntransform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0,0,0), (1,1,1)),])\n\n\ndataset = MyDataModule(root_dir='/kaggle/input/intel-image-classification/seg_train/seg_train/', transform=transform)\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)","metadata":{"_uuid":"30f18053-db6b-4d05-96c7-16299433f8b1","_cell_guid":"e729bef1-2e14-46e7-855b-08f5978569aa","_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-05-07T16:01:03.257634Z","iopub.execute_input":"2023-05-07T16:01:03.257946Z","iopub.status.idle":"2023-05-07T16:01:07.872262Z","shell.execute_reply.started":"2023-05-07T16:01:03.257895Z","shell.execute_reply":"2023-05-07T16:01:07.871491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A look at the real data distribution.","metadata":{}},{"cell_type":"code","source":"# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n\n# Plot some training images\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","metadata":{"_uuid":"d71ce3c6-94c8-47e7-b823-f56cf9ef7022","_cell_guid":"c4e95052-854b-45b4-b116-6ea2213b27cf","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-05-07T16:01:12.554612Z","iopub.execute_input":"2023-05-07T16:01:12.554902Z","iopub.status.idle":"2023-05-07T16:01:18.373475Z","shell.execute_reply.started":"2023-05-07T16:01:12.554850Z","shell.execute_reply":"2023-05-07T16:01:18.372701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Generator \n* We'll start with the Generator's network first. It takes random noise from latent space(of dim 100 in our case) and maps it to an image distribution(3x64x64). These fake images along with the real images from CIFAR-10 dataset are fed into the discriminator and it outputs the probability of the image being fake or real.\n* A method called [Upsampling](https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0) is used to produce images, we use ConvTranspose2d + stride that does the same work. Activations in every layer except the last layer is ReLu.\n* Batch Normalization stabilizes learning by normalizing the input to each unit to have zero mean and unit variance. This helps deal with training problems that arise due to poor initialization and helps the gradients flow in deeper models.\n* The architecture as given in the  [DCGAN paper](https://arxiv.org/abs/1511.06434).\n\n\n![](https://pytorch.org/tutorials/_images/dcgan_generator.png)","metadata":{"_uuid":"eec69624-fe2c-4a14-afc3-3c1a156d5e88","_cell_guid":"5aa6e940-7089-48ea-ac89-1a185f6b4c9a","trusted":true}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)","metadata":{"_uuid":"7e760fbc-69f2-4c8d-ad06-4ec45ecae6a8","_cell_guid":"7cab607c-0f75-4c29-85c7-4d3a4af71ac5","execution":{"iopub.status.busy":"2023-05-07T16:01:33.646893Z","iopub.execute_input":"2023-05-07T16:01:33.647541Z","iopub.status.idle":"2023-05-07T16:01:33.664291Z","shell.execute_reply.started":"2023-05-07T16:01:33.647225Z","shell.execute_reply":"2023-05-07T16:01:33.663449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The [DCGAN paper](https://arxiv.org/abs/1511.06434) mentions that that all model weights shall be randomly initialized from a Normal distribution with mean=0, stdev=0.02. These are applied to each layer of Generator and Discriminator.","metadata":{}},{"cell_type":"code","source":"# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T16:01:33.665832Z","iopub.execute_input":"2023-05-07T16:01:33.666280Z","iopub.status.idle":"2023-05-07T16:01:33.682239Z","shell.execute_reply.started":"2023-05-07T16:01:33.666087Z","shell.execute_reply":"2023-05-07T16:01:33.681555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the generator\nnetG = Generator(ngpu).to(device)\nnetG.apply(weights_init)\nprint(netG)","metadata":{"_uuid":"2f4105ce-c73a-4fe7-9906-d00ef405a882","_cell_guid":"1d04326f-ed46-4447-bf2a-7de4512b0dfb","execution":{"iopub.status.busy":"2023-05-07T16:01:33.683142Z","iopub.execute_input":"2023-05-07T16:01:33.683353Z","iopub.status.idle":"2023-05-07T16:01:33.734708Z","shell.execute_reply.started":"2023-05-07T16:01:33.683310Z","shell.execute_reply":"2023-05-07T16:01:33.733878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discriminator\n\n* The Discriminator is nothing but an Image classifier, that distinguishes images to be fake/real.\n* It has CNNs  with leaky ReLU activations. Many activation functions will work fine with this basic GAN architecture. However, leaky ReLUs are very popular because they help the gradients flow easier through the architecture.\n* Finally, it needs to output probabilities. We use a Sigmoid Activation for that.","metadata":{"_uuid":"19cafd32-ea6d-4b1c-9fa0-4c5312f7e339","_cell_guid":"ce9adca2-5226-4d89-a787-2a3bd0bcc803","trusted":true}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)","metadata":{"_uuid":"047c6291-9ccc-4a47-869a-362c396059ad","_cell_guid":"65edf924-ea88-49bf-940e-2b2894218433","execution":{"iopub.status.busy":"2023-05-07T16:01:33.735934Z","iopub.execute_input":"2023-05-07T16:01:33.736196Z","iopub.status.idle":"2023-05-07T16:01:33.756047Z","shell.execute_reply.started":"2023-05-07T16:01:33.736150Z","shell.execute_reply":"2023-05-07T16:01:33.755061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the Discriminator\nnetD = Discriminator(ngpu).to(device)\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.2.\nnetD.apply(weights_init)\n# Print the model\nprint(netD)","metadata":{"_uuid":"68e46959-880f-4eb6-8c68-697f86d393f0","_cell_guid":"4fe319ec-194a-431b-a3a8-802324ba54b8","execution":{"iopub.status.busy":"2023-05-07T16:01:33.757530Z","iopub.execute_input":"2023-05-07T16:01:33.758154Z","iopub.status.idle":"2023-05-07T16:01:33.792874Z","shell.execute_reply.started":"2023-05-07T16:01:33.758081Z","shell.execute_reply":"2023-05-07T16:01:33.792068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both the generator and discriminator are trained on Binary Cross Entropy Loss and Adam optimizer with same learning rates are used. These parameters are kept the same as the original paper except for Label Smoothing.","metadata":{"_uuid":"a40cb7ca-c1e8-4488-8535-d8c123baceb8","_cell_guid":"df21df24-0aac-46c8-ab88-84847d454cd3","trusted":true}},{"cell_type":"code","source":"# Initialize BCELoss function\ncriterion = nn.BCELoss()\n# Establish convention for real and fake labels during training\n# real_label = 1\n# fake_label = 0\n\"\"\"adding label smoothing\"\"\"\nreal_label=0.9\nfake_label=0.1\n\n# Setup Adam optimizers for both G and D\n\noptimizerD = optim.Adam(netD.parameters(), lr=d_lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=g_lr, betas=(beta1, 0.999))","metadata":{"_uuid":"77eab1a6-02c0-472a-b3ce-0f6da05344af","_cell_guid":"d277dbef-b7a4-4fab-b727-1ac29f6b61e1","execution":{"iopub.status.busy":"2023-05-07T16:01:33.794053Z","iopub.execute_input":"2023-05-07T16:01:33.794513Z","iopub.status.idle":"2023-05-07T16:01:33.800168Z","shell.execute_reply.started":"2023-05-07T16:01:33.794454Z","shell.execute_reply":"2023-05-07T16:01:33.799151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fretchet Inception Distance\nFID measures the distance between the Inception-v3 activation distributions for generated and real samples. But before looking into FID let's discuss a little about it's predecessor Inception Score.\n### Inception Score\n> The inception score computes the KL divergence between the conditional class distribution and the marginal class distribution.It measures the quality of generated images and their diversity.\n\nLet's first talk about entropy in the sense of a random variable or a probability distribution. Entropy can be viewed as randomness. If the value of a random variable x is highly predictable, it has low entropy. On the contrary, if it is highly unpredictable, the entropy is high.Now let's look at the equation to compute IS.\n![](https://miro.medium.com/max/576/1*UJ--WcXSxMKciCmHPqKlrw.png)\n> 1. **Conditional Probability**:  We want the conditional probability P(y|x) to be highly predictable (low entropy). i.e. given an image, we should know the object type easily. So we use an Inception network to classify the generated images and predict P(y|x) — where y is the label and x is the generated data. This reflects the **quality** of the images.\n> 2. **Marginal Probability:** The Marginal probability is computed as P(y)=\n![](https://miro.medium.com/max/576/1*7e3wKMaUPYvd6HDAtXUkqA.png)\nIf the generated images are diverse, the data distribution for y should be uniform (high entropy).\n\nOne shortcoming for IS is that it can misrepresent the performance if it only generates one image per class. p(y) will still be uniform even though the diversity is low.\n\n### Fretchet Inception Distance\nFID is a more principled and comprehensive metric and has been shown to be more consistent with human evaluation in assessing the realism and variation of the generated samples.\n\nThe calculation can be divided into three parts:\n\n1. We use the Inception network to extract 2048-dimensional activations from the pool3 layer for real and generated samples respectively.\n2. Then we model the data distribution for these features using a multivariate Gaussian distribution with mean µ and covariance Σ. The `calculate_activation_statistics()` function does this.\n3. Finally Wasserstein-2 distance is calculated for the mean and covariance of real images(x) and generated images(g).\n\n![](https://miro.medium.com/max/576/1*tJmwViZesuFM89TcVN7J3A.png) \n\n\n`calculate_fretchet_distance()` function does this.\n\n> **Lower FID means better quality and diversity.**\n\nThe implementation is from this amazing github repo https://github.com/mseitzer/pytorch-fid","metadata":{"_uuid":"b57dc087-2b7d-40d2-b461-6ad55910841f","_cell_guid":"79a164d9-0fa2-41ad-8b6b-191bdcb194b8","trusted":true}},{"cell_type":"code","source":"class InceptionV3(nn.Module):\n    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n\n    # Index of default block of inception to return,\n    # corresponds to output of final average pooling\n    DEFAULT_BLOCK_INDEX = 3\n\n    # Maps feature dimensionality to their output blocks indices\n    BLOCK_INDEX_BY_DIM = {\n        64: 0,   # First max pooling features\n        192: 1,  # Second max pooling featurs\n        768: 2,  # Pre-aux classifier features\n        2048: 3  # Final average pooling features\n    }\n\n    def __init__(self,\n                 output_blocks=[DEFAULT_BLOCK_INDEX],\n                 resize_input=True,\n                 normalize_input=True,\n                 requires_grad=False):\n        \n        super(InceptionV3, self).__init__()\n\n        self.resize_input = resize_input\n        self.normalize_input = normalize_input\n        self.output_blocks = sorted(output_blocks)\n        self.last_needed_block = max(output_blocks)\n\n        assert self.last_needed_block <= 3, \\\n            'Last possible output block index is 3'\n\n        self.blocks = nn.ModuleList()\n\n        \n        inception = models.inception_v3(pretrained=True)\n\n        # Block 0: input to maxpool1\n        block0 = [\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ]\n        self.blocks.append(nn.Sequential(*block0))\n\n        # Block 1: maxpool1 to maxpool2\n        if self.last_needed_block >= 1:\n            block1 = [\n                inception.Conv2d_3b_1x1,\n                inception.Conv2d_4a_3x3,\n                nn.MaxPool2d(kernel_size=3, stride=2)\n            ]\n            self.blocks.append(nn.Sequential(*block1))\n\n        # Block 2: maxpool2 to aux classifier\n        if self.last_needed_block >= 2:\n            block2 = [\n                inception.Mixed_5b,\n                inception.Mixed_5c,\n                inception.Mixed_5d,\n                inception.Mixed_6a,\n                inception.Mixed_6b,\n                inception.Mixed_6c,\n                inception.Mixed_6d,\n                inception.Mixed_6e,\n            ]\n            self.blocks.append(nn.Sequential(*block2))\n\n        # Block 3: aux classifier to final avgpool\n        if self.last_needed_block >= 3:\n            block3 = [\n                inception.Mixed_7a,\n                inception.Mixed_7b,\n                inception.Mixed_7c,\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n            ]\n            self.blocks.append(nn.Sequential(*block3))\n\n        for param in self.parameters():\n            param.requires_grad = requires_grad\n\n    def forward(self, inp):\n        \"\"\"Get Inception feature maps\n        Parameters\n        ----------\n        inp : torch.autograd.Variable\n            Input tensor of shape Bx3xHxW. Values are expected to be in\n            range (0, 1)\n        Returns\n        -------\n        List of torch.autograd.Variable, corresponding to the selected output\n        block, sorted ascending by index\n        \"\"\"\n        outp = []\n        x = inp\n\n        if self.resize_input:\n            x = F.interpolate(x,\n                              size=(299, 299),\n                              mode='bilinear',\n                              align_corners=False)\n\n        if self.normalize_input:\n            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n\n        for idx, block in enumerate(self.blocks):\n            x = block(x)\n            if idx in self.output_blocks:\n                outp.append(x)\n\n            if idx == self.last_needed_block:\n                break\n\n        return outp\n    \nblock_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\nmodel = InceptionV3([block_idx])\nmodel=model.cuda()","metadata":{"_uuid":"7f1f8a9d-9d8c-4467-92ec-262f86e723e7","_cell_guid":"a0c7ee8d-6743-4df2-82e4-8832ad3985b0","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-05-07T16:01:33.801510Z","iopub.execute_input":"2023-05-07T16:01:33.801950Z","iopub.status.idle":"2023-05-07T16:01:38.038851Z","shell.execute_reply.started":"2023-05-07T16:01:33.801736Z","shell.execute_reply":"2023-05-07T16:01:38.037821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n                    cuda=False):\n    model.eval()\n    act=np.empty((len(images), dims))\n    \n    if cuda:\n        batch=images.cuda()\n    else:\n        batch=images\n    pred = model(batch)[0]\n\n        # If model output is not scalar, apply global spatial average pooling.\n        # This happens if you choose a dimensionality not equal 2048.\n    if pred.size(2) != 1 or pred.size(3) != 1:\n        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n\n    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n    \n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma","metadata":{"_uuid":"a225156f-fbe8-42a6-956a-eba233a89369","_cell_guid":"7412d47e-36be-4a09-85e0-f8d0f032292b","execution":{"iopub.status.busy":"2023-05-07T16:01:38.040713Z","iopub.execute_input":"2023-05-07T16:01:38.041192Z","iopub.status.idle":"2023-05-07T16:01:38.047911Z","shell.execute_reply.started":"2023-05-07T16:01:38.040980Z","shell.execute_reply":"2023-05-07T16:01:38.046967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n    \"\"\"\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, \\\n        'Training and test mean vectors have different lengths'\n    assert sigma1.shape == sigma2.shape, \\\n        'Training and test covariances have different dimensions'\n\n    diff = mu1 - mu2\n\n    \n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        msg = ('fid calculation produces singular product; '\n               'adding %s to diagonal of cov estimates') % eps\n        print(msg)\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    \n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError('Imaginary component {}'.format(m))\n        covmean = covmean.real\n\n    tr_covmean = np.trace(covmean)\n\n    return (diff.dot(diff) + np.trace(sigma1) +\n            np.trace(sigma2) - 2 * tr_covmean)","metadata":{"_uuid":"27dad1e3-5d2f-4267-a3b3-cf452b38261f","_cell_guid":"77a80ded-a9a0-459a-93a9-cd2f5198a4fc","execution":{"iopub.status.busy":"2023-05-07T16:01:38.049181Z","iopub.execute_input":"2023-05-07T16:01:38.049757Z","iopub.status.idle":"2023-05-07T16:01:38.063691Z","shell.execute_reply.started":"2023-05-07T16:01:38.049696Z","shell.execute_reply":"2023-05-07T16:01:38.062745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_fretchet(images_real,images_fake,model):\n     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n    \n     \"\"\"get fretched distance\"\"\"\n     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n     return fid_value","metadata":{"_uuid":"5b10a58b-1336-4b8a-bc6e-fbbf8501c4ed","_cell_guid":"f937b8ca-534b-4575-a84a-17c198fb1fe6","execution":{"iopub.status.busy":"2023-05-07T16:01:38.064892Z","iopub.execute_input":"2023-05-07T16:01:38.065455Z","iopub.status.idle":"2023-05-07T16:01:38.078377Z","shell.execute_reply.started":"2023-05-07T16:01:38.065148Z","shell.execute_reply":"2023-05-07T16:01:38.077665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that everything is defined, let's start with the training, during which following steps are followed.\n1. Batch of Images(real and fake) are fed seperately in the Discriminator(a small trick called **Mininbatch Discrimination**). First the real images from CiFAR dataset are fed, loss is calculated and gradients are backpropogated, then the same thing follows for fake images output by the generator. Once that is done the Discriminator is updated.\n2. After that the Generator's cost is calculated based on the Discriminator's output. Then the Gradients are backpropogated and loss is calculated.\n3. This happens in a single step of GAN.\n\n> I'll mention some hacks that have worked for me so far:-\n* Normalizing the input between -1 and 1.\n* Label Smoothing.\n* Lowering the learning rate(when mode collapse happened).\n* Adding small noise to input to the discriminator.\n* Different Learning rate for discriminator and generator.\n\n> I've also tried the following things that didn't work quite well:-\n* Dropout in Generator, Leaky Relu in Genertor.\n* Less Generator updates for Discriminator updates.\n\nThere are some intuitions that you can follow while training GANs as mentioned in ganhacks:-\n* If the Discriminator loss approaches 0, it's a sign of failure.\n* If the loss of Generator steadily decreases, it's fooling the Discriminator with garbage.","metadata":{}},{"cell_type":"code","source":"print(\"Generator Parameters:\",sum(p.numel() for p in netG.parameters() if p.requires_grad))\nprint(\"Discriminator Parameters:\",sum(p.numel() for p in netD.parameters() if p.requires_grad))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-07T16:01:38.081041Z","iopub.execute_input":"2023-05-07T16:01:38.081560Z","iopub.status.idle":"2023-05-07T16:01:38.092705Z","shell.execute_reply.started":"2023-05-07T16:01:38.081289Z","shell.execute_reply":"2023-05-07T16:01:38.091836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nprint(\"Starting Training Loop...\")\nfor epoch in tqdm(range(num_epochs)):\n    # For each batch in the dataloader\n    for i, data in enumerate(dataloader, 0):\n        \n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        ## Train with all-real batch\n        netD.zero_grad()\n        # Format batch\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, device=device)\n#         # add some noise to the input to discriminator\n        \n        real_cpu=0.9*real_cpu+0.1*torch.randn((real_cpu.size()), device=device)\n        # Forward pass real batch through D\n        output = netD(real_cpu).view(-1)\n        # Calculate loss on all-real batch\n        errD_real = criterion(output, label)\n        # Calculate gradients for D in backward pass\n        errD_real.backward()\n        ## Train with all-fake batch\n        # Generate batch of latent vectors\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # Generate fake image batch with G\n        fake = netG(noise)\n        label.fill_(fake_label)\n        \n        fake=0.9*fake+0.1*torch.randn((fake.size()), device=device)\n        # Classify all fake batch with D\n        output = netD(fake.detach()).view(-1)\n        # Calculate D's loss on the all-fake batch\n        errD_fake = criterion(output, label)\n        # Calculate the gradients for this batch\n        errD_fake.backward()\n        # Add the gradients from the all-real and all-fake batches\n        errD = errD_real + errD_fake\n        # Update D\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        label.fill_(real_label)  # fake labels are real for generator cost\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        output = netD(fake).view(-1)\n        # Calculate G's loss based on this output\n        errG = criterion(output, label)\n        D_G_z2 = output.mean().item()\n        \n        # Calculate gradients for G\n        errG.backward()\n        # Update G\n        optimizerG.step()\n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fixed_noise = torch.randn(ngf, nz, 1, 1, device=device)\n                fake_display = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake_display, padding=2, normalize=True))\n            \n         \n            \n        iters += 1   \n    G_losses.append(errG.item())\n    D_losses.append(errD.item())     \n    fretchet_dist=calculate_fretchet(real_cpu,fake,model) \n    if ((epoch+1)%5==0):\n        \n        print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tFretchet_Distance: %.4f'\n                      % (epoch+1, num_epochs,\n                         errD.item(), errG.item(),fretchet_dist))\n        \n        \n        plt.figure(figsize=(8,8))\n        plt.axis(\"off\")\n        pictures=vutils.make_grid(fake_display[torch.randint(len(fake_display), (10,))],nrow=5,padding=2, normalize=True)\n        plt.imshow(np.transpose(pictures,(1,2,0)))\n        plt.show()","metadata":{"_uuid":"798e6985-a43c-45f8-86fd-9ce2db9ce5cb","_cell_guid":"0b0f7c89-5d18-4a3c-b42e-7952e0480b4b","execution":{"iopub.status.busy":"2023-05-07T16:01:38.094202Z","iopub.execute_input":"2023-05-07T16:01:38.094491Z","iopub.status.idle":"2023-05-07T17:25:51.164815Z","shell.execute_reply.started":"2023-05-07T16:01:38.094430Z","shell.execute_reply":"2023-05-07T17:25:51.163930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses,label=\"G\")\nplt.plot(D_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"_uuid":"2dd6e3cc-5122-4c81-b1a6-58e4126bc396","_cell_guid":"7a6f7ea3-e6b1-4d3f-8403-7e9c6ac3d4c1","execution":{"iopub.status.busy":"2023-05-07T17:25:51.166447Z","iopub.execute_input":"2023-05-07T17:25:51.166980Z","iopub.status.idle":"2023-05-07T17:25:51.515851Z","shell.execute_reply.started":"2023-05-07T17:25:51.166918Z","shell.execute_reply":"2023-05-07T17:25:51.514769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the images produced by the Genarator at the last iteration.\n","metadata":{}},{"cell_type":"code","source":"# Plot genearted images\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()","metadata":{"_uuid":"d7c252e1-e9a9-4330-9556-41a18a7852df","_cell_guid":"f34482e8-edc6-47e9-80f7-0d6e0db9ff3c","execution":{"iopub.status.busy":"2023-05-07T17:25:51.517254Z","iopub.execute_input":"2023-05-07T17:25:51.517618Z","iopub.status.idle":"2023-05-07T17:25:51.881467Z","shell.execute_reply.started":"2023-05-07T17:25:51.517560Z","shell.execute_reply":"2023-05-07T17:25:51.880728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(netD.state_dict(), \"generator.pth\")\ntorch.save(netG.state_dict(), \"discriminator.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:33:59.147712Z","iopub.execute_input":"2023-05-07T17:33:59.148022Z","iopub.status.idle":"2023-05-07T17:33:59.184824Z","shell.execute_reply.started":"2023-05-07T17:33:59.147956Z","shell.execute_reply":"2023-05-07T17:33:59.184030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can probably spot some birds, cars, trucks or dogs in the generated images.Some points I think are worth mentioning after my experiments are:-\n* It's hard to converge the model properly because we have a lot of classes(10). Images belonging to different classes have very different properties which is rather tricky for a single model to learn. \n* Conditional GAN might be a good approach to implement in this case. \n* Before concluding, I would like to mention that FID is sensitive to mode collapse.As mentioned in the paper [Are GANs created Equal?](https://arxiv.org/pdf/1711.10337.pdf), the metric increases for increasing modes.\n\n![](https://miro.medium.com/max/576/1*8PzOnrzIeuM0E1unrFKLfg.png)","metadata":{}},{"cell_type":"markdown","source":"## References\n\n* https://nealjean.com/ml/frechet-inception-distance/\n* https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n* https://github.com/soumith/ganhacks\n* https://github.com/mseitzer/pytorch-fid\n* https://medium.com/@jonathan_hui/gan-how-to-measure-gan-performance-64b988c47732","metadata":{"_uuid":"7207587a-e82d-4b6f-a6c7-fa6e97ce5f5d","_cell_guid":"93cb171d-0ade-45c8-a2d1-f82280074fda","trusted":true}},{"cell_type":"markdown","source":"***Please upvote the kernel, if you liked it.***\n\nI'm open to suggestions and feedback, please leave them in the comments below.","metadata":{}}]}