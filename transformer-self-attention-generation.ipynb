{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Transformer, Self-Attention и моделирование языка**","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt -q\nimport sys; sys.path.append('./stepik-dl-nlp')","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:09:42.102880Z","iopub.execute_input":"2021-07-25T21:09:42.103223Z","iopub.status.idle":"2021-07-25T21:12:05.292211Z","shell.execute_reply.started":"2021-07-25T21:09:42.103148Z","shell.execute_reply":"2021-07-25T21:12:05.291013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nimport youtokentome as yttm\n\nimport dlnlputils\nfrom dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n    save_texts_to_file, LanguageModelDataset, load_war_and_piece_chunks, \\\n    GreedyGenerator, BeamGenerator\nfrom dlnlputils.pipeline import train_eval_loop, init_random_seed\nfrom dlnlputils.base import get_params_number\n\ninit_random_seed()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:05.296276Z","iopub.execute_input":"2021-07-25T21:12:05.296590Z","iopub.status.idle":"2021-07-25T21:12:18.015837Z","shell.execute_reply.started":"2021-07-25T21:12:05.296552Z","shell.execute_reply":"2021-07-25T21:12:18.014663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Загрузка текстов и разбиение на обучающую и тестовую подвыборки**","metadata":{}},{"cell_type":"code","source":"all_chunks = load_war_and_piece_chunks('./stepik-dl-nlp/datasets/war_and_peace.txt')\nlen(all_chunks)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:18.018184Z","iopub.execute_input":"2021-07-25T21:12:18.018497Z","iopub.status.idle":"2021-07-25T21:12:18.123000Z","shell.execute_reply.started":"2021-07-25T21:12:18.018466Z","shell.execute_reply":"2021-07-25T21:12:18.121886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(all_chunks[10])","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:18.125254Z","iopub.execute_input":"2021-07-25T21:12:18.125701Z","iopub.status.idle":"2021-07-25T21:12:18.212879Z","shell.execute_reply.started":"2021-07-25T21:12:18.125657Z","shell.execute_reply":"2021-07-25T21:12:18.211537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.shuffle(all_chunks)\n\nTRAIN_SPLIT = int(len(all_chunks) * 0.7)\ntrain_texts = all_chunks[:TRAIN_SPLIT]\ntest_texts = all_chunks[TRAIN_SPLIT:]\n\nprint('Размер обучающей выборки', len(train_texts))\nprint('Размер валидационной выборки', len(test_texts))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:18.214623Z","iopub.execute_input":"2021-07-25T21:12:18.215481Z","iopub.status.idle":"2021-07-25T21:12:18.306137Z","shell.execute_reply.started":"2021-07-25T21:12:18.215432Z","shell.execute_reply":"2021-07-25T21:12:18.304551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Токенизация корпуса с помощью BPE**\n**BPE - Byte Pair Encoding**\n\n**YouTokenToMe - быстрая реализация BPE**","metadata":{}},{"cell_type":"code","source":"BPE_MODEL_FILENAME = './stepik-dl-nlp/models/war_and_peace_bpe.yttm'","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:18.308307Z","iopub.execute_input":"2021-07-25T21:12:18.309227Z","iopub.status.idle":"2021-07-25T21:12:18.392483Z","shell.execute_reply.started":"2021-07-25T21:12:18.309177Z","shell.execute_reply":"2021-07-25T21:12:18.391321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_TEXTS_FILENAME = './stepik-dl-nlp/datasets/war_and_peace_bpe_train.txt'\nsave_texts_to_file(train_texts, TRAIN_TEXTS_FILENAME)\nyttm.BPE.train(data=TRAIN_TEXTS_FILENAME, vocab_size=1000, model=BPE_MODEL_FILENAME);","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:18.394689Z","iopub.execute_input":"2021-07-25T21:12:18.395279Z","iopub.status.idle":"2021-07-25T21:12:18.841578Z","shell.execute_reply.started":"2021-07-25T21:12:18.395200Z","shell.execute_reply":"2021-07-25T21:12:18.840245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = yttm.BPE(BPE_MODEL_FILENAME)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:18.849659Z","iopub.execute_input":"2021-07-25T21:12:18.850084Z","iopub.status.idle":"2021-07-25T21:12:18.995466Z","shell.execute_reply.started":"2021-07-25T21:12:18.850042Z","shell.execute_reply":"2021-07-25T21:12:18.993887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(' '.join(tokenizer.vocab()))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:19.002590Z","iopub.execute_input":"2021-07-25T21:12:19.003203Z","iopub.status.idle":"2021-07-25T21:12:19.125475Z","shell.execute_reply.started":"2021-07-25T21:12:19.003156Z","shell.execute_reply":"2021-07-25T21:12:19.124254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.encode(train_texts[:1]))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:19.127318Z","iopub.execute_input":"2021-07-25T21:12:19.127822Z","iopub.status.idle":"2021-07-25T21:12:19.217404Z","shell.execute_reply.started":"2021-07-25T21:12:19.127754Z","shell.execute_reply":"2021-07-25T21:12:19.215692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_token_ids = tokenizer.encode(train_texts, bos=True, eos=True)\ntest_token_ids = tokenizer.encode(test_texts, bos=True, eos=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:19.219185Z","iopub.execute_input":"2021-07-25T21:12:19.219708Z","iopub.status.idle":"2021-07-25T21:12:19.497390Z","shell.execute_reply.started":"2021-07-25T21:12:19.219659Z","shell.execute_reply":"2021-07-25T21:12:19.495997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist([len(sent) for sent in train_token_ids], bins=30)\nplt.title('Распределение длин фрагментов в токенах')\nplt.yscale('log');","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:19.502522Z","iopub.execute_input":"2021-07-25T21:12:19.502922Z","iopub.status.idle":"2021-07-25T21:12:20.437268Z","shell.execute_reply.started":"2021-07-25T21:12:19.502890Z","shell.execute_reply":"2021-07-25T21:12:20.436194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_counts = np.bincount([token_id for text in train_token_ids for token_id in text])\nplt.hist(token_counts, bins=100)\nplt.title('Распределение количества упоминаний токенов')\nplt.yscale('log');","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:20.438932Z","iopub.execute_input":"2021-07-25T21:12:20.439386Z","iopub.status.idle":"2021-07-25T21:12:21.375147Z","shell.execute_reply.started":"2021-07-25T21:12:20.439340Z","shell.execute_reply":"2021-07-25T21:12:21.373799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unknown_subwords_in_test = sum(1 for text in test_token_ids for token_id in text if token_id == 1)\nprint('Количество случаев с неизвестными n-граммами символов в валидационной выборке',\n      unknown_subwords_in_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:21.376945Z","iopub.execute_input":"2021-07-25T21:12:21.377390Z","iopub.status.idle":"2021-07-25T21:12:21.473138Z","shell.execute_reply.started":"2021-07-25T21:12:21.377345Z","shell.execute_reply":"2021-07-25T21:12:21.471891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Подготовка датасетов для PyTorch**","metadata":{}},{"cell_type":"code","source":"CHUNK_LENGTH = 80\n\ntrain_dataset = LanguageModelDataset(train_token_ids,\n                                     chunk_length=CHUNK_LENGTH)\ntest_dataset = LanguageModelDataset(test_token_ids,\n                                    chunk_length=CHUNK_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:21.474876Z","iopub.execute_input":"2021-07-25T21:12:21.475608Z","iopub.status.idle":"2021-07-25T21:12:21.566256Z","shell.execute_reply.started":"2021-07-25T21:12:21.475560Z","shell.execute_reply":"2021-07-25T21:12:21.565022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:21.568306Z","iopub.execute_input":"2021-07-25T21:12:21.568880Z","iopub.status.idle":"2021-07-25T21:12:21.659028Z","shell.execute_reply.started":"2021-07-25T21:12:21.568834Z","shell.execute_reply":"2021-07-25T21:12:21.657382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(list(train_dataset[0]))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:21.661141Z","iopub.execute_input":"2021-07-25T21:12:21.661805Z","iopub.status.idle":"2021-07-25T21:12:21.752141Z","shell.execute_reply.started":"2021-07-25T21:12:21.661728Z","shell.execute_reply":"2021-07-25T21:12:21.750750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Общие классы и функции**\n### **Маска зависимостей**","metadata":{}},{"cell_type":"code","source":"def make_target_dependency_mask(length):\n    full_mask = torch.ones(length, length)\n    ignore_mask = torch.tril(full_mask) < 1\n    full_mask.masked_fill_(ignore_mask, float('-inf'))\n    full_mask.masked_fill_(~ignore_mask, 0)\n    return full_mask\n\nmake_target_dependency_mask(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:21.753908Z","iopub.execute_input":"2021-07-25T21:12:21.754817Z","iopub.status.idle":"2021-07-25T21:12:21.931536Z","shell.execute_reply.started":"2021-07-25T21:12:21.754713Z","shell.execute_reply":"2021-07-25T21:12:21.930484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Кодирование позиции**","metadata":{}},{"cell_type":"code","source":"def make_positional_encoding(max_length, embedding_size):\n    time = np.pi * torch.arange(0, max_length).float()\n    freq_dividers = torch.arange(1, embedding_size // 2 + 1).float()\n    inputs = time[:, None] / freq_dividers[None, :]\n    \n    result = torch.zeros(max_length, embedding_size)\n    result[:, 0::2] = torch.sin(inputs)\n    result[:, 1::2] = torch.cos(inputs)\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:21.933527Z","iopub.execute_input":"2021-07-25T21:12:21.934032Z","iopub.status.idle":"2021-07-25T21:12:22.025067Z","shell.execute_reply.started":"2021-07-25T21:12:21.933986Z","shell.execute_reply":"2021-07-25T21:12:22.023493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_pos_codes = make_positional_encoding(30, 30)\nplt.plot(sample_pos_codes[:, ::3].numpy());\nplt.gcf().set_size_inches((15, 5))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:22.029709Z","iopub.execute_input":"2021-07-25T21:12:22.030057Z","iopub.status.idle":"2021-07-25T21:12:22.402378Z","shell.execute_reply.started":"2021-07-25T21:12:22.030025Z","shell.execute_reply":"2021-07-25T21:12:22.400946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Основной класс - языковая модель**","metadata":{}},{"cell_type":"code","source":"class LanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_size, backbone, emb_dropout=0.0):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.emb_dropout = nn.Dropout(emb_dropout)\n        self.backbone = backbone\n        self.out = nn.Linear(embedding_size, vocab_size)\n    \n    def forward(self, seed_token_ids):\n        \"\"\"\n            seed_token_ids - BatchSize x MaxInLen\n        \"\"\"\n        batch_size, max_in_length = seed_token_ids.shape\n\n        seed_padding_mask = seed_token_ids == 0\n        dependency_mask = make_target_dependency_mask(max_in_length) \\\n            .to(seed_token_ids.device)\n        \n        seed_embs = self.embeddings(seed_token_ids)  # BatchSize x MaxInLen x EmbSize\n        pos_codes = make_positional_encoding(max_in_length,\n                                             self.embedding_size).unsqueeze(0).to(seed_embs.device)\n        seed_embs = seed_embs + pos_codes\n        seed_embs = self.emb_dropout(seed_embs)\n\n        # BatchSize x TargetLen x EmbSize\n        target_features = seed_embs\n        target_features = self.backbone(seed_embs,\n                                        mask=dependency_mask,\n                                        src_key_padding_mask=seed_padding_mask)\n        logits = self.out(target_features)  # BatchSize x TargetLen x VocabSize\n        return logits","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:22.404298Z","iopub.execute_input":"2021-07-25T21:12:22.404890Z","iopub.status.idle":"2021-07-25T21:12:22.512627Z","shell.execute_reply.started":"2021-07-25T21:12:22.404763Z","shell.execute_reply":"2021-07-25T21:12:22.511324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Утилиты для обучения - функция потерь и расписание изменения длины градиентного шага**","metadata":{}},{"cell_type":"code","source":"def lm_cross_entropy(pred, target):\n    \"\"\"\n    pred - BatchSize x TargetLen x VocabSize\n    target - BatchSize x TargetLen\n    \"\"\"\n    pred_flat = pred.view(-1, pred.shape[-1])  # BatchSize*TargetLen x VocabSize\n    target_flat = target.view(-1)  # BatchSize*TargetLen\n    return F.cross_entropy(pred_flat, target_flat, ignore_index=0)\n\n\ndef lr_scheduler(optimizer):\n    return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                      patience=20,\n                                                      factor=0.5,\n                                                      verbose=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:22.514543Z","iopub.execute_input":"2021-07-25T21:12:22.515121Z","iopub.status.idle":"2021-07-25T21:12:22.604380Z","shell.execute_reply.started":"2021-07-25T21:12:22.515063Z","shell.execute_reply":"2021-07-25T21:12:22.603330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Реализация Transformer из PyTorch 1.2**","metadata":{}},{"cell_type":"code","source":"class BatchFirstTransformerEncoder(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.impl = nn.TransformerEncoder(*args, **kwargs)\n        self.initialize_weights()\n    \n    def forward(self, src, *args, **kwargs):\n        src = src.transpose(0, 1).contiguous()  # MaxInLen  x BatchSize x EmbSize\n        result = self.impl(src, *args, **kwargs)  # TargetLen x BatchSize x EmbSize\n        result = result.transpose(0, 1).contiguous()  # BatchSize x TargetLen x EmbSize\n        return result\n    \n    def initialize_weights(self):\n        for param in self.impl.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:22.611132Z","iopub.execute_input":"2021-07-25T21:12:22.611519Z","iopub.status.idle":"2021-07-25T21:12:22.701682Z","shell.execute_reply.started":"2021-07-25T21:12:22.611487Z","shell.execute_reply":"2021-07-25T21:12:22.700552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_transf_model = LanguageModel(tokenizer.vocab_size(),\n                                   256,\n                                   BatchFirstTransformerEncoder(\n                                       nn.TransformerEncoderLayer(\n                                           d_model=256,\n                                           nhead=16,\n                                           dim_feedforward=512,\n                                           dropout=0.1),\n                                       num_layers=3),\n                                   emb_dropout=0.1)\nprint('Количество параметров', get_params_number(torch_transf_model))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:12:22.704588Z","iopub.execute_input":"2021-07-25T21:12:22.705363Z","iopub.status.idle":"2021-07-25T21:12:22.834916Z","shell.execute_reply.started":"2021-07-25T21:12:22.705315Z","shell.execute_reply":"2021-07-25T21:12:22.833523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(best_val_loss,\n best_torch_transf_model) = train_eval_loop(torch_transf_model,\n                                            train_dataset,\n                                            test_dataset,\n                                            lm_cross_entropy,\n                                            lr=2e-3,\n                                            epoch_n=200,\n                                            batch_size=512,\n                                            device='cuda',\n                                            early_stopping_patience=50,\n                                            max_batches_per_epoch_train=1000,\n                                            max_batches_per_epoch_val=1000,\n                                            lr_scheduler_ctor=lr_scheduler)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:57:26.735411Z","iopub.execute_input":"2021-07-25T21:57:26.735873Z","iopub.status.idle":"2021-07-25T22:07:13.228845Z","shell.execute_reply.started":"2021-07-25T21:57:26.735829Z","shell.execute_reply":"2021-07-25T22:07:13.227733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(best_torch_transf_model.state_dict(), './stepik-dl-nlp/models/war_and_peace_torch_transf_best.pth')","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:17:17.399007Z","iopub.execute_input":"2021-07-25T21:17:17.399444Z","iopub.status.idle":"2021-07-25T21:17:17.518430Z","shell.execute_reply.started":"2021-07-25T21:17:17.399401Z","shell.execute_reply":"2021-07-25T21:17:17.517285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_transf_model.load_state_dict(torch.load('./stepik-dl-nlp/models/war_and_peace_torch_transf_best.pth'))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:17:17.521858Z","iopub.execute_input":"2021-07-25T21:17:17.522294Z","iopub.status.idle":"2021-07-25T21:17:17.628025Z","shell.execute_reply.started":"2021-07-25T21:17:17.522252Z","shell.execute_reply":"2021-07-25T21:17:17.626607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Генерация текста с помощью языковой модели**\n### **Жадная генерация**","metadata":{}},{"cell_type":"code","source":"greedy_generator = GreedyGenerator(torch_transf_model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:17:17.629975Z","iopub.execute_input":"2021-07-25T21:17:17.630457Z","iopub.status.idle":"2021-07-25T21:17:17.720956Z","shell.execute_reply.started":"2021-07-25T21:17:17.630409Z","shell.execute_reply":"2021-07-25T21:17:17.719643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nprint(greedy_generator('сказала княжна, оглядывая Бона'))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:17:17.722888Z","iopub.execute_input":"2021-07-25T21:17:17.723437Z","iopub.status.idle":"2021-07-25T21:17:18.095850Z","shell.execute_reply.started":"2021-07-25T21:17:17.723368Z","shell.execute_reply":"2021-07-25T21:17:18.094728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(greedy_generator('смеялась княжна, оглядывая Наполе'))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:17:18.097376Z","iopub.execute_input":"2021-07-25T21:17:18.098163Z","iopub.status.idle":"2021-07-25T21:17:18.462367Z","shell.execute_reply.started":"2021-07-25T21:17:18.098082Z","shell.execute_reply":"2021-07-25T21:17:18.461047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(greedy_generator('сказала княжна, оглядывая Кутуз'))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:17:18.464027Z","iopub.execute_input":"2021-07-25T21:17:18.464688Z","iopub.status.idle":"2021-07-25T21:17:18.827158Z","shell.execute_reply.started":"2021-07-25T21:17:18.464642Z","shell.execute_reply":"2021-07-25T21:17:18.825736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(greedy_generator('сказал Кутузов, оглядывая Наполеона'))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:17:18.828952Z","iopub.execute_input":"2021-07-25T21:17:18.829696Z","iopub.status.idle":"2021-07-25T21:17:19.195232Z","shell.execute_reply.started":"2021-07-25T21:17:18.829647Z","shell.execute_reply":"2021-07-25T21:17:19.193022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Генерация с помощью лучевого поиска - Beam Search**","metadata":{}},{"cell_type":"code","source":"beam_generator = BeamGenerator(torch_transf_model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:17:19.196942Z","iopub.execute_input":"2021-07-25T21:17:19.197328Z","iopub.status.idle":"2021-07-25T21:17:19.291451Z","shell.execute_reply.started":"2021-07-25T21:17:19.197296Z","shell.execute_reply":"2021-07-25T21:17:19.290152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nbeam_gen_variants = beam_generator('сказала княжна, оглядывая Наполе',\n                                   beamsize=5,\n                                   return_hypotheses_n=5)\n\nfor score, pred_txt in beam_gen_variants:\n    print('****')\n    print(score)\n    print(pred_txt)\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:17:19.293623Z","iopub.execute_input":"2021-07-25T21:17:19.294110Z","iopub.status.idle":"2021-07-25T21:17:20.464029Z","shell.execute_reply.started":"2021-07-25T21:17:19.294067Z","shell.execute_reply":"2021-07-25T21:17:20.461929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nbeam_gen_variants = beam_generator('сказала княжна, оглядывая Наполе',\n                                   beamsize=20,\n                                   return_hypotheses_n=20)\n\nfor score, pred_txt in beam_gen_variants:\n    print('****')\n    print(score)\n    print(pred_txt)\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:17:20.465740Z","iopub.execute_input":"2021-07-25T21:17:20.466506Z","iopub.status.idle":"2021-07-25T21:17:27.903136Z","shell.execute_reply.started":"2021-07-25T21:17:20.466459Z","shell.execute_reply":"2021-07-25T21:17:27.902034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nbeam_gen_variants = beam_generator('сказала княжна, оглядывая Наполе',\n                                   beamsize=100,\n                                   return_hypotheses_n=20)\n\nfor score, pred_txt in beam_gen_variants:\n    print('****')\n    print(score)\n    print(pred_txt)\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:17:27.904653Z","iopub.execute_input":"2021-07-25T21:17:27.905098Z","iopub.status.idle":"2021-07-25T21:18:19.143291Z","shell.execute_reply.started":"2021-07-25T21:17:27.905053Z","shell.execute_reply":"2021-07-25T21:18:19.142049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Собственная реализация MultiHeadAttention**","metadata":{}},{"cell_type":"code","source":"def my_multihead_attention(queries, keys, values,\n                           keys_padding_mask, dependency_mask,\n                           is_training,\n                           weights_dropout):\n    \"\"\"\n    queries - BatchSize x ValuesLen x HeadN x KeySize\n    keys - BatchSize x KeysLen x HeadN x KeySize\n    values - BatchSize x KeysLen x HeadN x ValueSize\n    keys_padding_mask - BatchSize x KeysLen\n    dependency_mask - ValuesLen x KeysLen\n    is_training - bool\n    weights_dropout - float\n    \n    result - tuple of two:\n        - BatchSize x ValuesLen x HeadN x ValueSize - resulting features\n        - BatchSize x ValuesLen x KeysLen x HeadN - attention map\n    \"\"\"\n\n    # BatchSize x ValuesLen x KeysLen x HeadN\n    relevances = torch.einsum('bvhs,bkhs->bvkh', (queries, keys))\n    \n    # замаскировать элементы, выходящие за длины последовательностей ключей\n    padding_mask_expanded = keys_padding_mask[:, None, :, None].expand_as(relevances)\n    relevances.masked_fill_(padding_mask_expanded, float('-inf'))\n    \n    # замаскировать пары <выходная позиция, входная позиция>\n    relevances = relevances + dependency_mask[None, :, :, None].expand_as(relevances)\n    \n    normed_rels = F.softmax(relevances, dim=2)    \n    normed_rels = F.dropout(normed_rels, weights_dropout, is_training)\n    \n    # BatchSize x ValuesLen x KeysLen x HeadN x 1\n    normed_rels_expanded = normed_rels.unsqueeze(-1)\n    \n    # BatchSize x 1 x KeysLen x HeadN x ValueSize\n    values_expanded = values.unsqueeze(1)\n    \n    # BatchSize x ValuesLen x KeysLen x HeadN x ValueSize\n    weighted_values = normed_rels_expanded * values_expanded\n    result = weighted_values.sum(2)  # BatchSize x ValuesLen x HeadN x ValueSize\n    \n    return result, normed_rels","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:18:19.145583Z","iopub.execute_input":"2021-07-25T21:18:19.146141Z","iopub.status.idle":"2021-07-25T21:18:19.238397Z","shell.execute_reply.started":"2021-07-25T21:18:19.146098Z","shell.execute_reply":"2021-07-25T21:18:19.236878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Self-Attention - это Attention, в котором ключи, значения и запросы вычисляются из элементов одной и той же последовательности**","metadata":{}},{"cell_type":"code","source":"class MyMultiheadSelfAttention(nn.Module):\n    def __init__(self, model_size, n_heads, dropout=0):\n        super().__init__()\n        assert model_size % n_heads == 0, 'Размерность модели должна делиться нацело на количество голов'\n        self.n_heads = n_heads\n\n        self.queries_proj = nn.Linear(model_size, model_size)\n        self.keys_proj = nn.Linear(model_size, model_size)\n        self.values_proj = nn.Linear(model_size, model_size)\n        \n        self.dropout = dropout\n\n        self.last_attention_map = None\n    \n    def forward(self, sequence, padding_mask, dependency_mask):\n        \"\"\"\n        sequence - BatchSize x Len x ModelSize\n        padding_mask - BatchSize x Len\n        dependency_mask - Len x Len\n        \n        result - BatchSize x Len x ModelSize\n        \"\"\"\n        batch_size, max_len, model_size = sequence.shape\n        \n        queries_flat = self.queries_proj(sequence)  # BatchSize x Len x ModelSize\n        queries = queries_flat.view(batch_size, max_len, self.n_heads, -1)\n        \n        keys_flat = self.keys_proj(sequence)  # BatchSize x Len x ModelSize\n        keys = keys_flat.view(batch_size, max_len, self.n_heads, -1)\n        \n        values_flat = self.values_proj(sequence)  # BatchSize x Len x ModelSize\n        values = values_flat.view(batch_size, max_len, self.n_heads, -1)\n        \n        # BatchSize x Len x HeadsN x ValueSize\n        result, att_map = my_multihead_attention(queries, keys, values,\n                                                 padding_mask, dependency_mask,\n                                                 self.training, self.dropout)\n        result_flat = result.view(batch_size, max_len, model_size)\n        \n        self.last_attention_map = att_map.detach()\n\n        return result_flat","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:18:19.241973Z","iopub.execute_input":"2021-07-25T21:18:19.242420Z","iopub.status.idle":"2021-07-25T21:18:19.341013Z","shell.execute_reply.started":"2021-07-25T21:18:19.242371Z","shell.execute_reply":"2021-07-25T21:18:19.339727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Один слой трансформера - Self-Attention, Feed-Forward, skip-connections, LayerNorm**","metadata":{}},{"cell_type":"code","source":"class MyTransformerEncoderLayer(nn.Module):\n    def __init__(self, model_size, n_heads, dim_feedforward, dropout):\n        super().__init__()\n        self.self_attention = MyMultiheadSelfAttention(model_size,\n                                                       n_heads,\n                                                       dropout=dropout)\n        self.first_dropout = nn.Dropout(dropout)\n        self.first_norm = nn.LayerNorm(model_size)\n        \n        self.feedforward = nn.Sequential(\n            nn.Linear(model_size, dim_feedforward),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim_feedforward, model_size),\n            nn.Dropout(dropout)\n        )\n        self.second_norm = nn.LayerNorm(model_size)\n    \n    def forward(self, sequence, padding_mask, dependency_mask):\n        att_features = self.self_attention(sequence, padding_mask, dependency_mask)\n\n        sequence = sequence + self.first_dropout(att_features)\n        sequence = self.first_norm(sequence)\n        \n        sequence = sequence + self.feedforward(sequence)\n        sequence = self.second_norm(sequence)\n        return sequence","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:18:19.342960Z","iopub.execute_input":"2021-07-25T21:18:19.343511Z","iopub.status.idle":"2021-07-25T21:18:19.438529Z","shell.execute_reply.started":"2021-07-25T21:18:19.343455Z","shell.execute_reply":"2021-07-25T21:18:19.437401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Энкодер Трансформера - стопка из нескольких слоёв**","metadata":{}},{"cell_type":"code","source":"class MyTransformerEncoder(nn.Module):\n    def __init__(self, n_layers, **layer_kwargs):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            MyTransformerEncoderLayer(**layer_kwargs)\n            for _ in range(n_layers)\n        ])\n        self.initialize_weights()\n\n    def forward(self, sequence, mask, src_key_padding_mask):\n        for layer in self.layers:\n            sequence = layer(sequence, src_key_padding_mask, mask)\n        return sequence\n\n    def initialize_weights(self):\n        for param in self.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:18:19.441124Z","iopub.execute_input":"2021-07-25T21:18:19.441992Z","iopub.status.idle":"2021-07-25T21:18:19.534879Z","shell.execute_reply.started":"2021-07-25T21:18:19.441943Z","shell.execute_reply":"2021-07-25T21:18:19.533678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Попробуем обучить языковую модель с нашим Трансформером**","metadata":{}},{"cell_type":"code","source":"my_transf_model = LanguageModel(tokenizer.vocab_size(),\n                                256,\n                                MyTransformerEncoder(\n                                    n_layers=3,\n                                    model_size=256,\n                                    n_heads=16,\n                                    dim_feedforward=512,\n                                    dropout=0.1),\n                                emb_dropout=0.1)\nprint('Количество параметров', get_params_number(my_transf_model))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:18:19.536489Z","iopub.execute_input":"2021-07-25T21:18:19.537187Z","iopub.status.idle":"2021-07-25T21:18:19.664155Z","shell.execute_reply.started":"2021-07-25T21:18:19.537133Z","shell.execute_reply":"2021-07-25T21:18:19.662917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(best_val_loss,\n best_my_transf_model) = train_eval_loop(my_transf_model,\n                                         train_dataset,\n                                         test_dataset,\n                                         lm_cross_entropy,\n                                         lr=2e-3,\n                                         epoch_n=200,\n                                         batch_size=512,\n                                         device='cuda',\n                                         early_stopping_patience=50,\n                                         max_batches_per_epoch_train=1000,\n                                         max_batches_per_epoch_val=1000,\n                                         lr_scheduler_ctor=lr_scheduler)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T21:18:19.665886Z","iopub.execute_input":"2021-07-25T21:18:19.666541Z","iopub.status.idle":"2021-07-25T21:57:24.385554Z","shell.execute_reply.started":"2021-07-25T21:18:19.666496Z","shell.execute_reply":"2021-07-25T21:57:24.384529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntorch.save(best_my_transf_model.state_dict(), './stepik-dl-nlp/models/war_and_peace_my_transf_best.pth')","metadata":{"execution":{"iopub.status.busy":"2021-07-25T22:07:13.231656Z","iopub.execute_input":"2021-07-25T22:07:13.232154Z","iopub.status.idle":"2021-07-25T22:07:13.526214Z","shell.execute_reply.started":"2021-07-25T22:07:13.232085Z","shell.execute_reply":"2021-07-25T22:07:13.525019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_transf_model.load_state_dict(torch.load('./stepik-dl-nlp/models/war_and_peace_my_transf_best.pth'))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T22:08:13.265660Z","iopub.execute_input":"2021-07-25T22:08:13.266037Z","iopub.status.idle":"2021-07-25T22:08:13.368485Z","shell.execute_reply.started":"2021-07-25T22:08:13.266005Z","shell.execute_reply":"2021-07-25T22:08:13.367220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Наша реализация - жадная генерация**","metadata":{}},{"cell_type":"code","source":"my_greedy_generator = GreedyGenerator(my_transf_model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T22:08:15.057278Z","iopub.execute_input":"2021-07-25T22:08:15.057615Z","iopub.status.idle":"2021-07-25T22:08:15.142254Z","shell.execute_reply.started":"2021-07-25T22:08:15.057586Z","shell.execute_reply":"2021-07-25T22:08:15.141118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_greedy_generator('сказала княжна, оглядывая Андре')","metadata":{"execution":{"iopub.status.busy":"2021-07-25T22:09:04.493998Z","iopub.execute_input":"2021-07-25T22:09:04.494392Z","iopub.status.idle":"2021-07-25T22:09:04.804220Z","shell.execute_reply.started":"2021-07-25T22:09:04.494362Z","shell.execute_reply":"2021-07-25T22:09:04.802703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Визуализация карт внимания**","metadata":{}},{"cell_type":"code","source":"def plot_attention_maps(model, input_string, tokenizer, device='cuda', max_heads=2, figsize=(16, 10)):\n    device = torch.device(device)\n\n    token_ids = tokenizer.encode([input_string])[0]\n\n    token_strs = [tokenizer.id_to_subword(i) for i in token_ids]\n    in_len = len(token_ids)\n    ticks = np.arange(0, in_len)\n\n    model.to(device)\n    model.eval()\n\n    in_batch = torch.tensor(token_ids).unsqueeze(0).to(device)\n    model(in_batch)\n\n    for module in model.modules():\n        if isinstance(module, MyMultiheadSelfAttention):\n            cur_last_attention_map = module.last_attention_map[0].cpu().numpy()\n            n_heads = cur_last_attention_map.shape[-1]\n            n_heads_to_vis = min(n_heads, max_heads)\n\n            fig, axes = plt.subplots(1, n_heads_to_vis)\n            fig.set_size_inches(figsize)\n            for head_i in range(n_heads_to_vis):\n                ax = axes[head_i]\n                ax.imshow(cur_last_attention_map[..., head_i])\n\n                ax.set_yticks(ticks)\n                ax.set_ylim(bottom=in_len - 0.5, top=-0.5)\n                ax.set_yticklabels(token_strs)\n\n                ax.set_xticks(ticks)\n                ax.set_xticklabels(token_strs)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T22:08:20.976008Z","iopub.execute_input":"2021-07-25T22:08:20.976471Z","iopub.status.idle":"2021-07-25T22:08:21.067443Z","shell.execute_reply.started":"2021-07-25T22:08:20.976427Z","shell.execute_reply":"2021-07-25T22:08:21.066383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_attention_maps(my_transf_model, 'сказал Кутузов, оглядывая Бонапарта', tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T22:08:21.069845Z","iopub.execute_input":"2021-07-25T22:08:21.070270Z","iopub.status.idle":"2021-07-25T22:08:22.464170Z","shell.execute_reply.started":"2021-07-25T22:08:21.070231Z","shell.execute_reply":"2021-07-25T22:08:22.463162Z"},"trusted":true},"execution_count":null,"outputs":[]}]}