{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/khanhlvg/tflite_raspberry_pi/blob/main/object_detection/Train_custom_model_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"rPHUIJzLxUxU"}},{"cell_type":"markdown","source":"##### Copyright 2021 The TensorFlow Authors.","metadata":{"id":"gf2if_fGDaWc"}},{"cell_type":"code","source":"#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"cellView":"form","id":"jrmj83afDJrv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train a custom object detection model with TensorFlow Lite Model Maker\n\nIn this colab notebook, you'll learn how to use the [TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/guide/model_maker) to train a custom object detection model to detect Android figurines and how to put the model on a Raspberry Pi.\n\nThe Model Maker library uses *transfer learning* to simplify the process of training a TensorFlow Lite model using a custom dataset. Retraining a TensorFlow Lite model with your own custom dataset reduces the amount of training data required and will shorten the training time.\n","metadata":{"id":"PpJEzDG6DK2Q"}},{"cell_type":"markdown","source":"## Preparation\n\n### Install the required packages\nStart by installing the required packages, including the Model Maker package from the [GitHub repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) and the pycocotools library you'll use for evaluation.","metadata":{"id":"BRYjtwRZGBOI"}},{"cell_type":"code","source":"!pip install llvmlite\n%pip install -q tflite-model-maker\n%pip install -q tflite-support","metadata":{"id":"35BJmtVpAP_n","outputId":"0845c0b1-0b1a-40b8-a38d-d28e716d78b4","execution":{"iopub.status.busy":"2022-08-17T20:41:43.376219Z","iopub.execute_input":"2022-08-17T20:41:43.376906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tflite-model-maker-nightly","metadata":{"execution":{"iopub.status.busy":"2022-02-19T20:18:51.514380Z","iopub.execute_input":"2022-02-19T20:18:51.514669Z","iopub.status.idle":"2022-02-19T20:19:04.851027Z","shell.execute_reply.started":"2022-02-19T20:18:51.514642Z","shell.execute_reply":"2022-02-19T20:19:04.850274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import the required packages.","metadata":{"id":"prQ86DdtD317"}},{"cell_type":"code","source":"import numpy as np\nimport os\n\nfrom tflite_model_maker.config import ExportFormat, QuantizationConfig\nfrom tflite_model_maker import model_spec\nfrom tflite_model_maker import object_detector\n\nfrom tflite_support import metadata\n\nimport tensorflow as tf\nassert tf.__version__.startswith('2')\n\ntf.get_logger().setLevel('ERROR')\nfrom absl import logging\nlogging.set_verbosity(logging.ERROR)","metadata":{"id":"l4QQTXHHATDS","execution":{"iopub.status.busy":"2022-02-19T20:13:49.503912Z","iopub.execute_input":"2022-02-19T20:13:49.504348Z","iopub.status.idle":"2022-02-19T20:13:49.594713Z","shell.execute_reply.started":"2022-02-19T20:13:49.504307Z","shell.execute_reply":"2022-02-19T20:13:49.593129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the object detection model\n\n### Step 1: Load the dataset\n\n* Images in `train_data` is used to train the custom object detection model.\n* Images in `val_data` is used to check if the model can generalize well to new images that it hasn't seen before.","metadata":{"id":"Yxh3KInCFeB-"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/gdrive')\n!ln -s /content/gdrive/My\\ Drive/ /mydrive\n!ls /mydrive","metadata":{"id":"9e9ZW3sqMEPO","outputId":"2b897b68-7e2a-496e-8938-3229013ebbaa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /mydrive/freedomtech.zip","metadata":{"id":"bSk8i67yykbk","outputId":"0684aa1f-02e3-4d9e-bd35-7be3c9d9f630"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = object_detector.DataLoader.from_pascal_voc(\n    'freedomtech/train',\n    'freedomtech/train',\n    ['esp8266', 'pico']\n)\n\nval_data = object_detector.DataLoader.from_pascal_voc(\n    'freedomtech/validate',\n    'freedomtech/validate',\n    ['esp8266', 'pico']\n)","metadata":{"id":"WiAahdsQAdT7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2: Select a model architecture\n\nEfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n\nHere is the performance of each EfficientDet-Lite models compared to each others.\n\n| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n|--------------------|-----------|---------------|----------------------|\n| EfficientDet-Lite0 | 4.4       | 146           | 25.69%               |\n| EfficientDet-Lite1 | 5.8       | 259           | 30.55%               |\n| EfficientDet-Lite2 | 7.2       | 396           | 33.97%               |\n| EfficientDet-Lite3 | 11.4      | 716           | 37.70%               |\n| EfficientDet-Lite4 | 19.9      | 1886          | 41.96%               |\n\n<i> * Size of the integer quantized models. <br/>\n** Latency measured on Raspberry Pi 4 using 4 threads on CPU. <br/>\n*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n</i>\n\nIn this notebook, we use EfficientDet-Lite0 to train our model. You can choose other model architectures depending on whether speed or accuracy is more important to you.","metadata":{"id":"UNRhB8N7GHXj"}},{"cell_type":"code","source":"spec = model_spec.get('efficientdet_lite0')","metadata":{"id":"GZOojrDHAY1J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 3: Train the TensorFlow model with the training data.\n\n* Set `epochs = 20`, which means it will go through the training dataset 20 times. You can look at the validation accuracy during training and stop when you see validation loss (`val_loss`) stop decreasing to avoid overfitting.\n* Set `batch_size = 4` here so you will see that it takes 15 steps to go through the 62 images in the training dataset.\n* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model.","metadata":{"id":"5aeDU4mIM4ft"}},{"cell_type":"code","source":"model = object_detector.create(train_data, model_spec=spec, batch_size=4, train_whole_model=True, epochs=20, validation_data=val_data)","metadata":{"id":"_MClfpsJAfda","outputId":"de30aca3-527d-41e8-a80b-40fb331ce83c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 4. Evaluate the model with the validation data.\n\nAfter training the object detection model using the images in the training dataset, use the 10 images in the validation dataset to evaluate how the model performs against new data it has never seen before.\n\nAs the default batch size is 64, it will take 1 step to go through the 10 images in the validation dataset.\n\nThe evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval).","metadata":{"id":"KB4hKeerMmh4"}},{"cell_type":"code","source":"model.evaluate(val_data)","metadata":{"id":"OUqEpcYwAg8L","outputId":"c59f59f1-e97a-4489-c02d-844cfbfa1bc2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 5: Export as a TensorFlow Lite model.\n\nExport the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is [full integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant). This allows the TensorFlow Lite model to be smaller, run faster on Raspberry Pi CPU and also compatible with the Google Coral EdgeTPU.","metadata":{"id":"NARVYk9rGLIl"}},{"cell_type":"code","source":"model.export(export_dir='.', tflite_filename='android.tflite')","metadata":{"id":"_u3eFxoBAiqE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 6:  Evaluate the TensorFlow Lite model.\n\nSeveral factors can affect the model accuracy when exporting to TFLite:\n* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\nKeras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n\nTherefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model.","metadata":{"id":"JZcBmEigOCO3"}},{"cell_type":"markdown","source":"Before compiling the `.tflite` file for the Edge TPU, it's important to consider whether your model will fit into the Edge TPU memory. \n\nThe Edge TPU has approximately 8 MB of SRAM for [caching model paramaters](https://coral.ai/docs/edgetpu/compiler/#parameter-data-caching), so any model close to or over 8 MB will not fit onto the Edge TPU memory. That means the inference times are longer, because some model parameters must be fetched from the host system memory.\n\nOne way to elimiate the extra latency is to use [model pipelining](https://coral.ai/docs/edgetpu/pipeline/), which splits the model into segments that can run on separate Edge TPUs in series. This can significantly reduce the latency for big models.\n\nThe following table provides recommendations for the number of Edge TPUs to use with each EfficientDet-Lite model.\n\n| Model architecture | Minimum TPUs | Recommended TPUs\n|--------------------|-------|-------|\n| EfficientDet-Lite0 | 1     | 1     |\n| EfficientDet-Lite1 | 1     | 1     |\n| EfficientDet-Lite2 | 1     | 2     |\n| EfficientDet-Lite3 | 2     | 2     |\n| EfficientDet-Lite4 | 2     | 3     |\n\nIf you need extra Edge TPUs for your model, then update `NUMBER_OF_TPUS` here:","metadata":{"id":"rzF6u0FZTAjF"}},{"cell_type":"markdown","source":"Finally, we'll copy the metadata, including the label file, from the original TensorFlow Lite model to the EdgeTPU model.","metadata":{"id":"pJYXucYWTGqZ"}}]}